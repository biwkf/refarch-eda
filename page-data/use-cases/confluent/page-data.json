{"componentChunkName":"component---src-pages-use-cases-confluent-index-mdx","path":"/use-cases/confluent/","result":{"pageContext":{"frontmatter":{"title":"Getting started with Confluent Kafka with OpenShift","description":"Getting started with Confluent Kafka with OpenShift"},"relativePagePath":"/use-cases/confluent/index.mdx","titleType":"append","MdxNode":{"id":"2f6b4377-1c90-5f15-b35e-a17ee51f63a1","children":[],"parent":"3c2b55ad-2825-5b9e-8654-376960541075","internal":{"content":"---\ntitle: Getting started with Confluent Kafka with OpenShift\ndescription: Getting started with Confluent Kafka with OpenShift\n---\n\nIn this scenario, we're going to do a development deployment of Confluent platform using the Confluent for Kubernetes Operator.\nWe are using TLS encryption between each components, and configuring different listeners for authentication, and \nexpose the Kafka bootstrap server with OpenShift routes.\n\nTo validate the deployment we will use two simple applications to produce and consume order messages.\n\nConfluent has worked on the development of new operator. The current product documentation can be found [in Confluent for kubernetes](https://docs.confluent.io/operator/current/overview.html).\nSome examples of cluster configuration are in [the confluent-kubernetes-examples github repo](https://github.com/confluentinc/confluent-kubernetes-examples).\n\n## Prerequisites \n\n- OC CLI\n- Helm CLI\n- Helm 3 on OCP deployed\n- Cluster administrator access to OCP Cluster\n- Be sure clock synchronization is setup using Network Time Protocol between worker node (`ps -ef | grep ntpd`).\n- You can clone the eda-lab-inventory repository\n \n  ```sh\n    git clone https://github.com/ibm-cloud-architecture/eda-lab-inventory\n  ```\n\n## Assess platform sizing \n\nFor production deployment you can use the [eventsizer website](https://eventsizer.io/) to assess the cluster sizing for VM, \nbare metal or Kubernetes deployment. The [Confluent System requirements](https://docs.confluent.io/platform/current/installation/system-requirements.html#system-requirements) is also \ngiving guidelines for the different Confluent components in term of physical resources and supported OS.\n\nThe important considerations for the broker are the disk, RAM and CPUs. Dedicating one to one k8s worker node to Kafka broker node is a safe decision.\n\n## Installing Confluent for Kubernetes operator\n\n1. Log in to the OpenShift cluster via the CLI first: `oc login ...`\n2. Create a new project/namespace to encapsulate the Confluent for Kubernetes resources.\n\n    ```sh\n    oc new-project confluent\n    ```\n\n> If you change this project name then you need to update the yaml file to define the plaform (See [this example file](https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-inventory/master/environments/confluent/platform.yaml)).\n\n3. Add the necessary Confluent for Kubernetes **Helm** repository artifacts and update.\n\n    ```shell\n    helm repo add confluentinc https://packages.confluent.io/helm\n    helm repo update\n    ```\n\n4. Install the Operator.\n\n    ```shell\n    helm upgrade --install confluent-operator confluentinc/confluent-for-kubernetes\n    ```\n\n5. You can wait for the status with the following command\n\n    ```shell\n    oc wait deployment confluent-operator --for=condition=Available\n    ```\n\n6. Authorize service account to get privileged security context\n\n    **Note** - You should see an error with insufficient security context and that your deployment cannot create a pod. The reason for this is because the associated service account does not have the sufficient permissions to create the pod. Run the following command to get a list of the Service Accounts in the current namespace.\n\n    ```shell\n    oc get sa\n\n    NAME                       SECRETS   AGE\n    builder                    2         4m11s\n    confluent-for-kubernetes   2         3m4s\n    default                    2         4m11s\n    deployer                   2         4m11s\n    pipeline                   2         4m11s\n    ```\n\n   As you can see there's the `confluent-for-kubernetes` service account that was automatically created. Run the following command to give that service account the sufficient constraints.\n\n    ```sh\n    oc adm policy add-scc-to-user privileged -z confluent-for-kubernetes\n    ```\n    \n    **Note** - Instead of adding the `privileged` security context constraint to the `confluent-for-kubernetes` service account, you may fancy to change the UID (1001) in the Confluent for Kubernetes operator `values.yaml` files instead and applying those custom helm charts.\n\n7. You might need to start a new rollout manually.\n\n   ```shell\n   oc rollout latest deployment confluent-operator\n   ```\n\n\n## Deploying Confluent Platform\n\n### Security settings\n\nWe're going to be exposing the Kafka service using OpenShift routes which requires TLS configurations for Confluent Platform.\n\n* First of all, we will need to update the `default` Service Account so that the resources can be brought up.\n\n    ```shell\n    oc adm policy add-scc-to-user privileged -z default\n    ```\n\n> **Note** Similar to deploying the Confluent for Kubernetes operator, instead of adding the `privileged` security context constraint to the `default` \nservice account, you may fancy to change the UID (1001) required in the helm chart yaml files instead and applying those.\n\n* In the root of [one of our lab repository](https://github.com/ibm-cloud-architecture/eda-lab-inventory) is a folder named `environment/confluent/certs/`. \nWe have defined some sample configurations to use to \ngenerate our Certificate Authority (CA) keys. \n\n> **IMPORTANT** - If using another namespace name, the `certs/server-domain.json` file may need to be modified. \n\nYou can replace the `confluent` value with the project/namespace that you have your Confluent for Kubernetes resources located in.\nBelow is an example of \n\n```shell\n    *.confluent.svc.cluster.local,\n    *.zookeeper.confluent.svc.cluster.local,\n    *.kafka.confluent.svc.cluster.local\n```\n\n* We will need the [cfssl](https://github.com/cloudflare/cfssl) CLI tool to sign, verify and bundle TLS certificates. On MacOS you can use brew to install it.\n\n    ```sh\n    brew install cfssl\n    ```\n\n* We'll create a new folder `certs/generated/` to keep the CA generated files.\n\n ```sh\n    # under environments/confluent\n    mkdir ./certs/generated\n    cfssl gencert -initca ./certs/ca-csr.json | cfssljson -bare ./certs/generated/ca -\n ```\n\n this should creates the following files:\n\n ```sh\n   ├── generated\n  │   ├── ca-key.pem\n  │   ├── ca.csr\n   │   └── ca.pem\n ```\n\n* We can validate the CA file with the following.\n\n ```sh\n    openssl x509 -in ./certs/generated/ca.pem -text -noout\n ```\n\n* Now to create server certificates with the appropriate SANs (SANs listed in server-domain.json) we do:\n\n ```sh\n    cfssl gencert -ca=./certs/generated/ca.pem \\\n    -ca-key=./certs/generated/ca-key.pem \\\n    -config=./certs/ca-config.json \\\n    -profile=server ./certs/server-domain.json | cfssljson -bare ./certs/generated/server \n ```\n\n  This should add:\n\n  ```sh\n    ├── generated\n    |  ├── server-key.pem\n  |  ├── server.csr\n    |  └── server.pem \n  ```\n\n* Again, we can validate the server certificate and SANs\n\n ```sh\n    openssl x509 -in ./certs/generated/server.pem -text -noout\n ```\n\n* We're going to create eight OpenShift secrets which will all use the same CA files for the sake of simplicity. In production we may is differnet certificate\nfor each component. They will all be named differently as the TLS configurations within the various Custom Resources expect different TLS secrets. \n\n ```sh\n    oc create secret generic generic-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic kafka-tls-internal \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic kafka-tls-external \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem && \n    oc create secret generic zk-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem && \n    oc create secret generic connect-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic ksqldb-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic controlcenter-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic schemaregistry-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem \n ```\n\n* We need to create another secret for the internal Kafka listener so that the other Confluent Platform resources can connect to the Kafka cluster\n over PLAINTEXT.\n\n    ```sh\n    oc create secret generic internal-plain-credential \\\n    --from-file=plain-users.json=./certs/creds-kafka-sasl-users.json \\\n    --from-file=plain.txt=./certs/creds-client-kafka-sasl-user.txt\n    ```\n\n \n### Deploy the platform components form one descriptor\n\n\n* Get the Openshift ingress subdomain name: \n\n ![Ingress Subdomain](./images/cluster-subdomain.png)\n\n and update the `environments/confluent/platform.yaml` file to reflect this domain for the route elements:\n\n ```yaml\n  apiVersion: platform.confluent.io/v1beta1\n  kind: Kafka \n  spec:\n    listeners: \n        external:\n            externalAccess:\n                type: route\n                route:\n                    domain: <tochange>.....\n\n  # control center\n  apiVersion: platform.confluent.io/v1beta1\n  kind: ControlCenter\n  spec:\n    externalAccess:\n        type: route\n        route:\n            domain: <tochange>.\n ```\n\n* Apply the minimum custom resources to get 3 zookeepers, 3 Kafka Broker, 1 Kafka connector, ksqlDB, ControlCenter and schema registry:\n\n    ```shell\n    oc apply -f environments/confluent/platform.yaml\n    ```\n\n> **Note** - If you created a namespace with a name other than `confluent` you will need to create a local yaml file and you can either remove `metadata.namespace: confluent` \nin each of the Custom Resource YAMLs and apply that file in your created namespace or edit `metadata.namespace: ` value to your created one. \nYou can also customize the settings in these YAMLs as you see fit. \nIn the [eda-lab-inventory repository](https://github.com/ibm-cloud-architecture/eda-lab-inventory) you will find [such a file](https://github.com/ibm-cloud-architecture/eda-lab-inventory/tree/master/environments/confluent/platform.yaml).\n\n* Now wait a few minutes for all the resources to come up.\n\n    ```shell\n    oc get pods -w\n\n    NAME                                  READY   STATUS    RESTARTS   AGE\n    confluent-operator-5b4fb58d99-vbnsn   1/1     Running   0          29m\n    connect-0                             1/1     Running   2          5m44s\n    controlcenter-0                       0/1     Pending   0          108s\n    kafka-0                               1/1     Running   0          4m9s\n    kafka-1                               1/1     Running   0          4m9s\n    kafka-2                               1/1     Running   0          4m9s\n    ksqldb-0                              0/1     Pending   0          109s\n    schemaregistry-0                      1/1     Running   0          109s\n    zookeeper-0                           1/1     Running   0          5m44s\n    zookeeper-1                           1/1     Running   0          5m44s\n    zookeeper-2                           1/1     Running   0          5m44s\n    ```\n\n* Once everything is ready you can test by port-forwarding to `controlcenter-0` pod.\n\n    ```shell\n    oc port-forward controlcenter-0 9021:9021\n    ```\n\n    Forwarding from 127.0.0.1:9021 -> 9021\n    Forwarding from [::1]:9021 -> 9021\n    Handling connection for 9021\n\n* In your browser go to `localhost:9021`.\n\n    ![Confluent Control Center](./images/confluent-control-center.png)\n\n\n## Add topics\n\nCreate the `KafkaTopic` custom resource named `orders`.\n\n ```sh\ncat << EOF | oc apply -f -\napiVersion: platform.confluent.io/v1beta1\nkind: KafkaTopic\nmetadata:\n  name: orders\n  namespace: confluent\nspec:\n  replicas: 3\n  partitionCount: 1\n  configs:\n    cleanup.policy: \"delete\"\nEOF\n\n ```\n\n## Validate deployment by access control center user interface\n\nOnce everything is up and running you can verify the Control center using a port forward like before.\n\n```shell\noc port-forward control-center-0 9021:9021\n```\n\n* Go to `https://localhost:9021`. The `https` is important unlike previously as it's now secured.\n\nYou can also view the Control Center UI from the enabled route which will be in the form of something like the following\n`cc-route......containers.appdomain.cloud`\n","type":"Mdx","contentDigest":"0c6daa14bbe0608f487f0475cfc24a05","owner":"gatsby-plugin-mdx","counter":865},"frontmatter":{"title":"Getting started with Confluent Kafka with OpenShift","description":"Getting started with Confluent Kafka with OpenShift"},"exports":{},"rawBody":"---\ntitle: Getting started with Confluent Kafka with OpenShift\ndescription: Getting started with Confluent Kafka with OpenShift\n---\n\nIn this scenario, we're going to do a development deployment of Confluent platform using the Confluent for Kubernetes Operator.\nWe are using TLS encryption between each components, and configuring different listeners for authentication, and \nexpose the Kafka bootstrap server with OpenShift routes.\n\nTo validate the deployment we will use two simple applications to produce and consume order messages.\n\nConfluent has worked on the development of new operator. The current product documentation can be found [in Confluent for kubernetes](https://docs.confluent.io/operator/current/overview.html).\nSome examples of cluster configuration are in [the confluent-kubernetes-examples github repo](https://github.com/confluentinc/confluent-kubernetes-examples).\n\n## Prerequisites \n\n- OC CLI\n- Helm CLI\n- Helm 3 on OCP deployed\n- Cluster administrator access to OCP Cluster\n- Be sure clock synchronization is setup using Network Time Protocol between worker node (`ps -ef | grep ntpd`).\n- You can clone the eda-lab-inventory repository\n \n  ```sh\n    git clone https://github.com/ibm-cloud-architecture/eda-lab-inventory\n  ```\n\n## Assess platform sizing \n\nFor production deployment you can use the [eventsizer website](https://eventsizer.io/) to assess the cluster sizing for VM, \nbare metal or Kubernetes deployment. The [Confluent System requirements](https://docs.confluent.io/platform/current/installation/system-requirements.html#system-requirements) is also \ngiving guidelines for the different Confluent components in term of physical resources and supported OS.\n\nThe important considerations for the broker are the disk, RAM and CPUs. Dedicating one to one k8s worker node to Kafka broker node is a safe decision.\n\n## Installing Confluent for Kubernetes operator\n\n1. Log in to the OpenShift cluster via the CLI first: `oc login ...`\n2. Create a new project/namespace to encapsulate the Confluent for Kubernetes resources.\n\n    ```sh\n    oc new-project confluent\n    ```\n\n> If you change this project name then you need to update the yaml file to define the plaform (See [this example file](https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-inventory/master/environments/confluent/platform.yaml)).\n\n3. Add the necessary Confluent for Kubernetes **Helm** repository artifacts and update.\n\n    ```shell\n    helm repo add confluentinc https://packages.confluent.io/helm\n    helm repo update\n    ```\n\n4. Install the Operator.\n\n    ```shell\n    helm upgrade --install confluent-operator confluentinc/confluent-for-kubernetes\n    ```\n\n5. You can wait for the status with the following command\n\n    ```shell\n    oc wait deployment confluent-operator --for=condition=Available\n    ```\n\n6. Authorize service account to get privileged security context\n\n    **Note** - You should see an error with insufficient security context and that your deployment cannot create a pod. The reason for this is because the associated service account does not have the sufficient permissions to create the pod. Run the following command to get a list of the Service Accounts in the current namespace.\n\n    ```shell\n    oc get sa\n\n    NAME                       SECRETS   AGE\n    builder                    2         4m11s\n    confluent-for-kubernetes   2         3m4s\n    default                    2         4m11s\n    deployer                   2         4m11s\n    pipeline                   2         4m11s\n    ```\n\n   As you can see there's the `confluent-for-kubernetes` service account that was automatically created. Run the following command to give that service account the sufficient constraints.\n\n    ```sh\n    oc adm policy add-scc-to-user privileged -z confluent-for-kubernetes\n    ```\n    \n    **Note** - Instead of adding the `privileged` security context constraint to the `confluent-for-kubernetes` service account, you may fancy to change the UID (1001) in the Confluent for Kubernetes operator `values.yaml` files instead and applying those custom helm charts.\n\n7. You might need to start a new rollout manually.\n\n   ```shell\n   oc rollout latest deployment confluent-operator\n   ```\n\n\n## Deploying Confluent Platform\n\n### Security settings\n\nWe're going to be exposing the Kafka service using OpenShift routes which requires TLS configurations for Confluent Platform.\n\n* First of all, we will need to update the `default` Service Account so that the resources can be brought up.\n\n    ```shell\n    oc adm policy add-scc-to-user privileged -z default\n    ```\n\n> **Note** Similar to deploying the Confluent for Kubernetes operator, instead of adding the `privileged` security context constraint to the `default` \nservice account, you may fancy to change the UID (1001) required in the helm chart yaml files instead and applying those.\n\n* In the root of [one of our lab repository](https://github.com/ibm-cloud-architecture/eda-lab-inventory) is a folder named `environment/confluent/certs/`. \nWe have defined some sample configurations to use to \ngenerate our Certificate Authority (CA) keys. \n\n> **IMPORTANT** - If using another namespace name, the `certs/server-domain.json` file may need to be modified. \n\nYou can replace the `confluent` value with the project/namespace that you have your Confluent for Kubernetes resources located in.\nBelow is an example of \n\n```shell\n    *.confluent.svc.cluster.local,\n    *.zookeeper.confluent.svc.cluster.local,\n    *.kafka.confluent.svc.cluster.local\n```\n\n* We will need the [cfssl](https://github.com/cloudflare/cfssl) CLI tool to sign, verify and bundle TLS certificates. On MacOS you can use brew to install it.\n\n    ```sh\n    brew install cfssl\n    ```\n\n* We'll create a new folder `certs/generated/` to keep the CA generated files.\n\n ```sh\n    # under environments/confluent\n    mkdir ./certs/generated\n    cfssl gencert -initca ./certs/ca-csr.json | cfssljson -bare ./certs/generated/ca -\n ```\n\n this should creates the following files:\n\n ```sh\n   ├── generated\n  │   ├── ca-key.pem\n  │   ├── ca.csr\n   │   └── ca.pem\n ```\n\n* We can validate the CA file with the following.\n\n ```sh\n    openssl x509 -in ./certs/generated/ca.pem -text -noout\n ```\n\n* Now to create server certificates with the appropriate SANs (SANs listed in server-domain.json) we do:\n\n ```sh\n    cfssl gencert -ca=./certs/generated/ca.pem \\\n    -ca-key=./certs/generated/ca-key.pem \\\n    -config=./certs/ca-config.json \\\n    -profile=server ./certs/server-domain.json | cfssljson -bare ./certs/generated/server \n ```\n\n  This should add:\n\n  ```sh\n    ├── generated\n    |  ├── server-key.pem\n  |  ├── server.csr\n    |  └── server.pem \n  ```\n\n* Again, we can validate the server certificate and SANs\n\n ```sh\n    openssl x509 -in ./certs/generated/server.pem -text -noout\n ```\n\n* We're going to create eight OpenShift secrets which will all use the same CA files for the sake of simplicity. In production we may is differnet certificate\nfor each component. They will all be named differently as the TLS configurations within the various Custom Resources expect different TLS secrets. \n\n ```sh\n    oc create secret generic generic-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic kafka-tls-internal \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic kafka-tls-external \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem && \n    oc create secret generic zk-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem && \n    oc create secret generic connect-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic ksqldb-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic controlcenter-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem &&\n    oc create secret generic schemaregistry-tls \\\n    --from-file=fullchain.pem=./certs/generated/server.pem \\\n    --from-file=cacerts.pem=./certs/generated/ca.pem \\\n    --from-file=privkey.pem=./certs/generated/server-key.pem \n ```\n\n* We need to create another secret for the internal Kafka listener so that the other Confluent Platform resources can connect to the Kafka cluster\n over PLAINTEXT.\n\n    ```sh\n    oc create secret generic internal-plain-credential \\\n    --from-file=plain-users.json=./certs/creds-kafka-sasl-users.json \\\n    --from-file=plain.txt=./certs/creds-client-kafka-sasl-user.txt\n    ```\n\n \n### Deploy the platform components form one descriptor\n\n\n* Get the Openshift ingress subdomain name: \n\n ![Ingress Subdomain](./images/cluster-subdomain.png)\n\n and update the `environments/confluent/platform.yaml` file to reflect this domain for the route elements:\n\n ```yaml\n  apiVersion: platform.confluent.io/v1beta1\n  kind: Kafka \n  spec:\n    listeners: \n        external:\n            externalAccess:\n                type: route\n                route:\n                    domain: <tochange>.....\n\n  # control center\n  apiVersion: platform.confluent.io/v1beta1\n  kind: ControlCenter\n  spec:\n    externalAccess:\n        type: route\n        route:\n            domain: <tochange>.\n ```\n\n* Apply the minimum custom resources to get 3 zookeepers, 3 Kafka Broker, 1 Kafka connector, ksqlDB, ControlCenter and schema registry:\n\n    ```shell\n    oc apply -f environments/confluent/platform.yaml\n    ```\n\n> **Note** - If you created a namespace with a name other than `confluent` you will need to create a local yaml file and you can either remove `metadata.namespace: confluent` \nin each of the Custom Resource YAMLs and apply that file in your created namespace or edit `metadata.namespace: ` value to your created one. \nYou can also customize the settings in these YAMLs as you see fit. \nIn the [eda-lab-inventory repository](https://github.com/ibm-cloud-architecture/eda-lab-inventory) you will find [such a file](https://github.com/ibm-cloud-architecture/eda-lab-inventory/tree/master/environments/confluent/platform.yaml).\n\n* Now wait a few minutes for all the resources to come up.\n\n    ```shell\n    oc get pods -w\n\n    NAME                                  READY   STATUS    RESTARTS   AGE\n    confluent-operator-5b4fb58d99-vbnsn   1/1     Running   0          29m\n    connect-0                             1/1     Running   2          5m44s\n    controlcenter-0                       0/1     Pending   0          108s\n    kafka-0                               1/1     Running   0          4m9s\n    kafka-1                               1/1     Running   0          4m9s\n    kafka-2                               1/1     Running   0          4m9s\n    ksqldb-0                              0/1     Pending   0          109s\n    schemaregistry-0                      1/1     Running   0          109s\n    zookeeper-0                           1/1     Running   0          5m44s\n    zookeeper-1                           1/1     Running   0          5m44s\n    zookeeper-2                           1/1     Running   0          5m44s\n    ```\n\n* Once everything is ready you can test by port-forwarding to `controlcenter-0` pod.\n\n    ```shell\n    oc port-forward controlcenter-0 9021:9021\n    ```\n\n    Forwarding from 127.0.0.1:9021 -> 9021\n    Forwarding from [::1]:9021 -> 9021\n    Handling connection for 9021\n\n* In your browser go to `localhost:9021`.\n\n    ![Confluent Control Center](./images/confluent-control-center.png)\n\n\n## Add topics\n\nCreate the `KafkaTopic` custom resource named `orders`.\n\n ```sh\ncat << EOF | oc apply -f -\napiVersion: platform.confluent.io/v1beta1\nkind: KafkaTopic\nmetadata:\n  name: orders\n  namespace: confluent\nspec:\n  replicas: 3\n  partitionCount: 1\n  configs:\n    cleanup.policy: \"delete\"\nEOF\n\n ```\n\n## Validate deployment by access control center user interface\n\nOnce everything is up and running you can verify the Control center using a port forward like before.\n\n```shell\noc port-forward control-center-0 9021:9021\n```\n\n* Go to `https://localhost:9021`. The `https` is important unlike previously as it's now secured.\n\nYou can also view the Control Center UI from the enabled route which will be in the form of something like the following\n`cc-route......containers.appdomain.cloud`\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/confluent/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}