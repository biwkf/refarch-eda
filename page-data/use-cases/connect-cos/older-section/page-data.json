{"componentChunkName":"component---src-pages-use-cases-connect-cos-older-section-md","path":"/use-cases/connect-cos/older-section/","result":{"pageContext":{"frontmatter":{"description":"Use Cases / Connect Cos / Older Section","title":"Use Cases / Connect Cos / Older Section"},"relativePagePath":"/use-cases/connect-cos/older-section.md","titleType":"append","MdxNode":{"id":"ba15ee33-9967-5103-b9ef-b945d36fb5ef","children":[],"parent":"28d06ebd-153c-53ab-86e6-d56518a1de7d","internal":{"content":"## Set up the Kafka Connect Cluster\n\nIn this section, we are going to see how to deploy a [Kafka Connect](https://kafka.apache.org/documentation/#connect) cluster on OpenShift which will be the engine running the source and sink connector we decide to use for our use case. **IMPORTANT:** We assume you have deployed your IBM Event Streams instance with an **internal TLS secured listener** which your Kafka Connect cluster will use to connect. For more detail about listeners, check the IBM Event Streams documentation [here](https://ibm.github.io/event-streams/installing/configuring/#kafka-access).\n\nIf you inspect your IBM Event Streams instance by executing the following command:\n\n```shell\noc get EventStreams YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME -o yaml\n```\n\nYou should see a `TlS` listener:\n\n![connect5](./images/connect-5.png)\n\nNow, follow the next steps in order to get your Kafka Connect cluster deployed:\n\n1. Go to you IBM Event Streams dashboard, click on the `Find more on the toolbox` option.\n\n  ![Toolbox1](./images/connect-1.png)\n\n1. Click on the `Set up` button for the `Set up a Kafka Connect environment` option.\n\n  ![Toolbox2](./images/connect-2.png)\n\n1. Click on `Download Kafka Connect ZIP` button.\n\n  ![Toolbox3](./images/connect-3.png)\n\n1. The above downloads a zip file which contains a `kafka-connect-s2i.yaml` file. Open that yaml file and take note of the `productID` and `cloudpakId` values as you will need these in the following step.\n\n  ![connect4](./images/connect-4.png)\n\n1. Instead of using the previous yaml file, create a new `kafka-connect-s2i.yaml` file with the following contents:\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1beta1\n  kind: KafkaConnectS2I\n  metadata:\n    name: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n    annotations:\n      eventstreams.ibm.com/use-connector-resources: \"true\"\n  spec:\n    logging:\n      type: external\n      name: custom-connect-log4j\n    version: 2.6.0\n    replicas: 1\n    bootstrapServers: YOUR_INTERNAL_BOOTSTRAP_ADDRESS\n    template:\n      pod:\n        imagePullSecrets: []\n        metadata:\n          annotations:\n            eventstreams.production.type: CloudPakForIntegrationNonProduction\n            productID: YOUR_PRODUCT_ID\n            productName: IBM Event Streams for Non Production\n            productVersion: 10.2.0\n            productMetric: VIRTUAL_PROCESSOR_CORE\n            productChargedContainers: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n            cloudpakId: YOUR_CLOUDPAK_ID\n            cloudpakName: IBM Cloud Pak for Integration\n            cloudpakVersion: 2020.4.1\n            productCloudpakRatio: \"2:1\"\n    tls:\n        trustedCertificates:\n          - secretName: YOUR_CLUSTER_TLS_CERTIFICATE_SECRET\n            certificate: ca.crt\n    authentication:\n      type: tls\n      certificateAndKey:\n        certificate: user.crt\n        key: user.key\n        secretName: YOUR_TLS_CREDENTIALS_SECRET\n    config:\n      group.id: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n      key.converter: org.apache.kafka.connect.json.JsonConverter\n      value.converter: org.apache.kafka.connect.json.JsonConverter\n      key.converter.schemas.enable: false\n      value.converter.schemas.enable: false\n      offset.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-offsets\n      config.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-configs\n      status.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-status\n      config.storage.replication.factor: 1\n      offset.storage.replication.factor: 1\n      status.storage.replication.factor: 1\n  ```\n\n  where you will need to replace the following placeholders with the appropriate values for you IBM Event Streams cluster and service credentials:\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: A name you want to provide your Kafka Connect cluster and resources with.\n   * `YOUR_INTERNAL_BOOTSTRAP_ADDRESS`: This is the internal bootstrap address of your IBM Event Streams instance. You can review how to find this url [here](/use-cases/overview/pre-requisites#get-kafka-bootstrap-url). Use the **internal** bootstrap address which should be in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-kafka-bootstrap.eventstreams.svc:9093`:\n   ![connect6](./images/connect-6.png)\n   * `YOUR_TLS_CREDENTIALS_SECRET`: This is the name you give to your TLS credentials for your internal IBM Event Streams listener when you click on `Generate TLS credentials`:\n   ![connect7](./images/connect-7.png)\n   * `YOUR_CLUSTER_TLS_CERTIFICATE_SECRET`: This is the secret name where IBM Event Streams stores the TLS certificate for establishing secure communications. This secret name is in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-cluster-ca-cert`. You can always use `oc get secrets` to list all the secrets.\n   * `YOUR_PRODUCT_ID`: This is the `productID` value you noted down earlier.\n   * `YOUR_CLOUDPAK_ID`: This is the `cloudpakID` value you noted earlier.  \n\n1. Deploy your Kafka Connect cluster by executing\n\n  ```shell\n  oc apply -f kafkaconnect-s2i.yaml \n  ```\n\n1. If you list the pods, you should see three new pods: one for the Kafka Connect build task, another for the Kafka Connect deploy task and the actual Kafka Connect cluster pod.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-xxxxx      1/1     Running     0          17m\n  ```\n\n  ## Build and Inject IBM COS Sink Connector\n\nThe IBM COS Sink Connector source code is availabe at this repository [here](https://github.com/ibm-messaging/kafka-connect-ibmcos-sink).\n\n**IMPORTANT:** Make sure you have **Java 8** installed on your workstation and that is the default Java version of your system since the IBM COS Sink Connector can only be built with that version of Java.\n\n\n1. Clone the Kafka Connect IBM COS Source Connector repository and then change your folder.\n\n  ```shell\n  git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink.git\n  cd kafka-connect-ibmcos-sink/\n  ```\n  \n**IMPORTANT Part Two:** Depending on your Gradle version you have installed on your machine you will need to update the connector's gradle build file. For example Gradle v7.x the `build.gradle` file should look something like this as the `compile()` method is deprecated in newer versions. You can downgrade your Gradle version if you so choose. This is the [shadowJar repository](https://github.com/johnrengelman/shadow) for versioning information.\n\n```\n/*\n * Copyright 2019 IBM Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\nplugins {\n  id 'com.github.johnrengelman.shadow' version '4.0.3'\n  id 'java'\n  id 'eclipse'\n}\n\nrepositories {\n   mavenCentral()\n}\n\ndependencies {\n  implementation 'com.eclipsesource.minimal-json:minimal-json:0.9.5'\n  implementation 'com.ibm.cos:ibm-cos-java-sdk:2.4.5'\n  implementation 'org.apache.kafka:connect-api:2.2.1'\n\n  testImplementation 'org.mockito:mockito-all:1.10.19'\n  testImplementation 'junit:junit:4.12'\n  testImplementation 'com.squareup.okhttp3:mockwebserver:3.9.+'\n}\n\neclipse.project {\n  natures 'org.springsource.ide.eclipse.gradle.core.nature'\n}\n\nshadowJar {\n   classifier = 'all'\n   version = '7.0.0'\n}\n\n```\n\n\n1. Build the connect using `Gradle`.\n\n  ```shell\n  gradle shadowJar\n  ```\n\n1. The newly built connector binaries are in the `build/libs/` folder. Move it into a `connectors` folder for ease of use.\n\n  ```shell\n  mkdir connectors\n  cp build/libs/kafka-connect-ibmcos-sink-*-all.jar connectors/\n  ```\n\n1. Now that we have the connector in the `connectors/` folder, we somehow need embed it into our Kakfa Connect cluster. For that, we need to trigger another build for our Kafka Connect cluster but this time specifying the files we want to get embedded. What the followin command does is it builds a new image with your provided connectors/plugins and triggers a new deployment for your Kafka Connect cluster.\n\n  ```shell\n  oc start-build connect-cluster-101-connect --from-dir ./connectors/ --follow\n  ```\n\n1. Since the last commands triggers a new build, we should now see three new pods for the build task, the deploy task and the resulting Kafka Connect cluster. Also, we should see the previous Kafka Connect cluster pod if gone.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx      1/1     Running     0          17m\n  ```\n\n\n\n## Connect with mTLS \n\nUpdate the `applications.properties` file if you are not connecting to Event Streams via PLAINTEXT security protocol.\n\n  ```properties\n\n  quarkus.http.port=8080\n  quarkus.log.console.enable=true\n  quarkus.log.console.level=INFO\n\n  # Event Streams Connection details\n  mp.messaging.connector.smallrye-kafka.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  mp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\n  mp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\n  mp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\n  mp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                  username=REPLACE_WITH_YOUR_SCRAM_USERNAME \\\n                  password=REPLACE_WITH_YOUR_SCRAM_PASSWORD;\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.location=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n\n  # Initial mock JSON message producer configuration\n  mp.messaging.outgoing.INBOUND.connector=smallrye-kafka\n  mp.messaging.outgoing.INBOUND.topic=REPLACE_WITH_YOUR_TOPIC\n  mp.messaging.outgoing.INBOUND.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  mp.messaging.outgoing.INBOUND.key.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  ```\n\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION`: The location where you downloaded your PCKS12 TLS certificate to.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above.\n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n","type":"Mdx","contentDigest":"7375ca94d9fc32806962a6020dbb39f9","owner":"gatsby-plugin-mdx","counter":876},"frontmatter":{"description":"Use Cases / Connect Cos / Older Section","title":"Use Cases / Connect Cos / Older Section"},"exports":{},"rawBody":"## Set up the Kafka Connect Cluster\n\nIn this section, we are going to see how to deploy a [Kafka Connect](https://kafka.apache.org/documentation/#connect) cluster on OpenShift which will be the engine running the source and sink connector we decide to use for our use case. **IMPORTANT:** We assume you have deployed your IBM Event Streams instance with an **internal TLS secured listener** which your Kafka Connect cluster will use to connect. For more detail about listeners, check the IBM Event Streams documentation [here](https://ibm.github.io/event-streams/installing/configuring/#kafka-access).\n\nIf you inspect your IBM Event Streams instance by executing the following command:\n\n```shell\noc get EventStreams YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME -o yaml\n```\n\nYou should see a `TlS` listener:\n\n![connect5](./images/connect-5.png)\n\nNow, follow the next steps in order to get your Kafka Connect cluster deployed:\n\n1. Go to you IBM Event Streams dashboard, click on the `Find more on the toolbox` option.\n\n  ![Toolbox1](./images/connect-1.png)\n\n1. Click on the `Set up` button for the `Set up a Kafka Connect environment` option.\n\n  ![Toolbox2](./images/connect-2.png)\n\n1. Click on `Download Kafka Connect ZIP` button.\n\n  ![Toolbox3](./images/connect-3.png)\n\n1. The above downloads a zip file which contains a `kafka-connect-s2i.yaml` file. Open that yaml file and take note of the `productID` and `cloudpakId` values as you will need these in the following step.\n\n  ![connect4](./images/connect-4.png)\n\n1. Instead of using the previous yaml file, create a new `kafka-connect-s2i.yaml` file with the following contents:\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1beta1\n  kind: KafkaConnectS2I\n  metadata:\n    name: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n    annotations:\n      eventstreams.ibm.com/use-connector-resources: \"true\"\n  spec:\n    logging:\n      type: external\n      name: custom-connect-log4j\n    version: 2.6.0\n    replicas: 1\n    bootstrapServers: YOUR_INTERNAL_BOOTSTRAP_ADDRESS\n    template:\n      pod:\n        imagePullSecrets: []\n        metadata:\n          annotations:\n            eventstreams.production.type: CloudPakForIntegrationNonProduction\n            productID: YOUR_PRODUCT_ID\n            productName: IBM Event Streams for Non Production\n            productVersion: 10.2.0\n            productMetric: VIRTUAL_PROCESSOR_CORE\n            productChargedContainers: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n            cloudpakId: YOUR_CLOUDPAK_ID\n            cloudpakName: IBM Cloud Pak for Integration\n            cloudpakVersion: 2020.4.1\n            productCloudpakRatio: \"2:1\"\n    tls:\n        trustedCertificates:\n          - secretName: YOUR_CLUSTER_TLS_CERTIFICATE_SECRET\n            certificate: ca.crt\n    authentication:\n      type: tls\n      certificateAndKey:\n        certificate: user.crt\n        key: user.key\n        secretName: YOUR_TLS_CREDENTIALS_SECRET\n    config:\n      group.id: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n      key.converter: org.apache.kafka.connect.json.JsonConverter\n      value.converter: org.apache.kafka.connect.json.JsonConverter\n      key.converter.schemas.enable: false\n      value.converter.schemas.enable: false\n      offset.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-offsets\n      config.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-configs\n      status.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-status\n      config.storage.replication.factor: 1\n      offset.storage.replication.factor: 1\n      status.storage.replication.factor: 1\n  ```\n\n  where you will need to replace the following placeholders with the appropriate values for you IBM Event Streams cluster and service credentials:\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: A name you want to provide your Kafka Connect cluster and resources with.\n   * `YOUR_INTERNAL_BOOTSTRAP_ADDRESS`: This is the internal bootstrap address of your IBM Event Streams instance. You can review how to find this url [here](/use-cases/overview/pre-requisites#get-kafka-bootstrap-url). Use the **internal** bootstrap address which should be in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-kafka-bootstrap.eventstreams.svc:9093`:\n   ![connect6](./images/connect-6.png)\n   * `YOUR_TLS_CREDENTIALS_SECRET`: This is the name you give to your TLS credentials for your internal IBM Event Streams listener when you click on `Generate TLS credentials`:\n   ![connect7](./images/connect-7.png)\n   * `YOUR_CLUSTER_TLS_CERTIFICATE_SECRET`: This is the secret name where IBM Event Streams stores the TLS certificate for establishing secure communications. This secret name is in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-cluster-ca-cert`. You can always use `oc get secrets` to list all the secrets.\n   * `YOUR_PRODUCT_ID`: This is the `productID` value you noted down earlier.\n   * `YOUR_CLOUDPAK_ID`: This is the `cloudpakID` value you noted earlier.  \n\n1. Deploy your Kafka Connect cluster by executing\n\n  ```shell\n  oc apply -f kafkaconnect-s2i.yaml \n  ```\n\n1. If you list the pods, you should see three new pods: one for the Kafka Connect build task, another for the Kafka Connect deploy task and the actual Kafka Connect cluster pod.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-xxxxx      1/1     Running     0          17m\n  ```\n\n  ## Build and Inject IBM COS Sink Connector\n\nThe IBM COS Sink Connector source code is availabe at this repository [here](https://github.com/ibm-messaging/kafka-connect-ibmcos-sink).\n\n**IMPORTANT:** Make sure you have **Java 8** installed on your workstation and that is the default Java version of your system since the IBM COS Sink Connector can only be built with that version of Java.\n\n\n1. Clone the Kafka Connect IBM COS Source Connector repository and then change your folder.\n\n  ```shell\n  git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink.git\n  cd kafka-connect-ibmcos-sink/\n  ```\n  \n**IMPORTANT Part Two:** Depending on your Gradle version you have installed on your machine you will need to update the connector's gradle build file. For example Gradle v7.x the `build.gradle` file should look something like this as the `compile()` method is deprecated in newer versions. You can downgrade your Gradle version if you so choose. This is the [shadowJar repository](https://github.com/johnrengelman/shadow) for versioning information.\n\n```\n/*\n * Copyright 2019 IBM Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\nplugins {\n  id 'com.github.johnrengelman.shadow' version '4.0.3'\n  id 'java'\n  id 'eclipse'\n}\n\nrepositories {\n   mavenCentral()\n}\n\ndependencies {\n  implementation 'com.eclipsesource.minimal-json:minimal-json:0.9.5'\n  implementation 'com.ibm.cos:ibm-cos-java-sdk:2.4.5'\n  implementation 'org.apache.kafka:connect-api:2.2.1'\n\n  testImplementation 'org.mockito:mockito-all:1.10.19'\n  testImplementation 'junit:junit:4.12'\n  testImplementation 'com.squareup.okhttp3:mockwebserver:3.9.+'\n}\n\neclipse.project {\n  natures 'org.springsource.ide.eclipse.gradle.core.nature'\n}\n\nshadowJar {\n   classifier = 'all'\n   version = '7.0.0'\n}\n\n```\n\n\n1. Build the connect using `Gradle`.\n\n  ```shell\n  gradle shadowJar\n  ```\n\n1. The newly built connector binaries are in the `build/libs/` folder. Move it into a `connectors` folder for ease of use.\n\n  ```shell\n  mkdir connectors\n  cp build/libs/kafka-connect-ibmcos-sink-*-all.jar connectors/\n  ```\n\n1. Now that we have the connector in the `connectors/` folder, we somehow need embed it into our Kakfa Connect cluster. For that, we need to trigger another build for our Kafka Connect cluster but this time specifying the files we want to get embedded. What the followin command does is it builds a new image with your provided connectors/plugins and triggers a new deployment for your Kafka Connect cluster.\n\n  ```shell\n  oc start-build connect-cluster-101-connect --from-dir ./connectors/ --follow\n  ```\n\n1. Since the last commands triggers a new build, we should now see three new pods for the build task, the deploy task and the resulting Kafka Connect cluster. Also, we should see the previous Kafka Connect cluster pod if gone.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx      1/1     Running     0          17m\n  ```\n\n\n\n## Connect with mTLS \n\nUpdate the `applications.properties` file if you are not connecting to Event Streams via PLAINTEXT security protocol.\n\n  ```properties\n\n  quarkus.http.port=8080\n  quarkus.log.console.enable=true\n  quarkus.log.console.level=INFO\n\n  # Event Streams Connection details\n  mp.messaging.connector.smallrye-kafka.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  mp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\n  mp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\n  mp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\n  mp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                  username=REPLACE_WITH_YOUR_SCRAM_USERNAME \\\n                  password=REPLACE_WITH_YOUR_SCRAM_PASSWORD;\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.location=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n\n  # Initial mock JSON message producer configuration\n  mp.messaging.outgoing.INBOUND.connector=smallrye-kafka\n  mp.messaging.outgoing.INBOUND.topic=REPLACE_WITH_YOUR_TOPIC\n  mp.messaging.outgoing.INBOUND.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  mp.messaging.outgoing.INBOUND.key.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  ```\n\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION`: The location where you downloaded your PCKS12 TLS certificate to.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above.\n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-cos/older-section.md"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}