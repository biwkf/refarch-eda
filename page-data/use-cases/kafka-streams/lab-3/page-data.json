{"componentChunkName":"component---src-pages-use-cases-kafka-streams-lab-3-index-mdx","path":"/use-cases/kafka-streams/lab-3/","result":{"pageContext":{"frontmatter":{"title":"Kafka Streams Test Lab 3","description":"Using Kafka Streams to compute real time inventory stock"},"relativePagePath":"/use-cases/kafka-streams/lab-3/index.mdx","titleType":"append","MdxNode":{"id":"77708b60-f77b-5b8e-8070-251c4cbab58a","children":[],"parent":"2c7111bc-107e-5563-a59a-ee1cee267695","internal":{"content":"---\ntitle: Kafka Streams Test Lab 3\ndescription: Using Kafka Streams to compute real time inventory stock\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Work in progress</strong> Updated 06/09/2021\n</InlineNotification>\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Pre-requisites</AnchorLink>\n    <AnchorLink>One Click Deploy to OpenShift</AnchorLink>\n    <AnchorLink>Testing the solution</AnchorLink>\n    <AnchorLink>Understanding the Kafka Streams implementation</AnchorLink>\n    <AnchorLink>Interactive queries</AnchorLink>\n    <AnchorLink>Event Streams on Cloud as Kafka provider</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n\nIn this lab, we're going to use [Quarkus](https://quarkus.io) to develop the near real-time inventory logic using Kafka Streams APIs and microprofile reactive messaging.\n\nThe requirements to address are:\n\n- consume item sold events from the `items` topic. Item has SKU as unique key. Item event has store ID reference\n- the Kafka record in the `items` topic, uses the Store unique ID as key\n- compute for each item its current stock cross stores\n- compute the store's stock for each item\n- generate inventory event for store - item - stock\n- expose APIs to get stock for a store or for an item\n\n![0](./images/inventory-use-cases.png)\n\nThe solution is using Kafka Streams and it includes two services. The components used are:\n\n ![1](./images/inventory-components.png)\n\nThe goal of this lab, is to develop the green components which expose APIs to support Kafka Streams interactive query on top of the aggregates to keep\nthe inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic).\n\nWe will be unit testing the stream logic using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TopologyTestDriver class. \n\nThis solution is deployed to OpenShift cluster with Strimzi running in the same cluster and namespace.\n\nThis application needs the [Item Store sell simulator](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) to perform the end to end \ntesting and to demonstrate the end to end scenario.\n\n## Pre-requisites\n\n* Access to an **OpenShift Container Platform** v4.6.x\n* **Kafka**: The lab can use Event Streams on Cloud or Kafka Strimzi deployed on OpenShift.\n* **Code Source**: from the git repositories: \n  \n    * [https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory) to compute item inventory cross store\n    * [https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory) to compute store inventory.\n    * [https://github.com/ibm-cloud-architecture/eda-lab-inventory](https://github.com/ibm-cloud-architecture/eda-lab-inventory) to get access to deployment configuration to deploy on OpenShift.\n\n  ```sh \n  git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory\n  git clone https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory\n  git clone https://github.com/ibm-cloud-architecture/eda-lab-inventory\n  ```\n* OC CLI\n* As in previous labs, be connected to your Openshift cluster.\n\nWe have automated the deployment of all the pieces making up this use case. We are using the open source Strimzi operator to get a Kafka cluster deployed\n\n### IBM OpenLabs\n\nIn this section, we are going to see use the [IBM OpenLabs](https://developer.ibm.com/openlabs/openshift) hosted environment.\n\n1. Go to [IBM OpenLabs](https://developer.ibm.com/openlabs/openshift) in a browser and click on `Launch Lab` button for **Bring Your Own Application**.\n\n  ![OpenLabs1](images/openLabs1.png)\n\n1. Sign in with your IBM Cloud account or register for an IBM Cloud account.\n\n  ![OpenLabs2](images/openLabs2.png)\n\n1. You will be presented with a dialog asking you whether you have an **Opportunity Id** or not. \nIf you don't have it or don't no, just select **No** and click on **Launch Lab**.\n\n1. You should now see your IBM OpenLabs environment.\n\n  ![OpenLabs4](images/openLabs4.png)\n\n1. On the left hand side navigation menu, click on the **Quick Links and Common Commands** section. \nNow, if you scroll down on the instructions shown on your screen, you should reach the **Commonly Used Commands** \nsection of these and in there you should see an `oc login ...` command to get your terminal associated to this\n IBM OpenLabs logged into the OpenShift cluster that you will be working with for this quickstart tutorial. \nClick on the `oc login...` command and you should see a `Login successful` message on the terminal.\n\n  ![OpenLabs8](images/openLabs8.png)\n\n\n## One Click Deploy to OpenShift\n\nThe different components are deployed in the same namespace as the Kafka cluster, and use internal route to access Kafka bootstrap URL.\n\nThe images for each of the components used are in the [quay.io ibmcase](https://quay.io/organization/ibmcase) repository:\n\n![](./images/quay-images.png)\n\n1. Under the `eda-lab-inventory` folder open the `scripts/env-strimzi.sh` file to provide your own values or use our defined values:\n  \n  * **KAFKA_CLUSTER_NAME** which is the name you want to give to the Kafka cluster that will get deployed. It defaults to my-kafka and we highly recommend you to leave the default. Otherwise, you would need to modify some yaml deployment scripts…\n  * **YOUR_PROJECT_NAME** which is the name of the OpenShift project the one-click-deployment script will create for you to get all the components of this solution deployed into. It defaults to rt-inventory. \n\n1. Execute the `deployInventoryWithStrimzi.sh` one-click-deployment script:\n\n   ```sh\n   ./scripts/deployInventoryWithStrimzi.sh --skip-login\n   ```\n\n  This script can be run multiple times, in case of failure, it should continue creating the needed resources and apps.\n\n1. After some time (can easily take up to more than 10 min) you should see the following message:\n\n\n  ```sh\n  ********************\n  ** CONGRATULATIONS!! You have successfully deployed the realtime inventory use case. \n  ********************\n  ```\n1. Verify the running pods\n\n  ```sh\n  NAME                                        READY   STATUS    RESTARTS   AGE\n  item-aggregator-557db4c9c6-xds24            1/1     Running   0          8m47s\n  my-kafka-cruise-control-69dd479bf6-bmnt6    2/2     Running   0          10m\n  my-kafka-entity-operator-7d5f948d84-9knnn   3/3     Running   0          11m\n  my-kafka-kafka-0                            1/1     Running   0          12m\n  my-kafka-kafka-1                            1/1     Running   0          12m\n  my-kafka-kafka-2                            1/1     Running   0          12m\n  my-kafka-zookeeper-0                        1/1     Running   0          13m\n  my-kafka-zookeeper-1                        1/1     Running   0          13m\n  my-kafka-zookeeper-2                        1/1     Running   0          12m\n  store-aggregator-7df98556ff-rdhcw           1/1     Running   0          4m59s\n  store-simulator-56f8958498-44q9q            1/1     Running   0          9m30s\n  ```\n\n1. Get the different service endpoints:\n\n  ```sh\n  oc get routes\n\n  item-aggregator            item-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud            \n  my-kafka-kafka-bootstrap   my-kafka-kafka-bootstrap-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud   \n  store-aggregator           store-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud           \n  store-simulator            store-simulator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud \n  ```\n\n1. You can also access the OpenShift Console and go to the installed Operators and select Strimzi. In this operators you can use the cluster, the users and the topics:\n\n  ![](./images/strimzi-topics.png)\n\n### Some explanations of this deployment script\n\nThe script performs the following steps:\n\n* Create a project if it does not exist\n* Create a service account for the solution and a role binding to cluster role views\n* Create Strimzi operator if it is not already there\n* Create a kafka cluster in the project with 3 brokers and zookeeper\n* Create scram and tls user\n* Create the topics for solution\n* Create one config map to define topics and broker boostrap shareable between apps\n* Deploy each microservices\n\n## Testing the solution\n\nWe have moved the demonstration script to the [scenario chapter](/scenarios/realtime-inventory/#demonstration-script-for-the-solution).\n\n\n## Understanding the Kafka Streams implementation\n\nThe item and store aggregator code are based on the same code structure, reflecting the DDD onion architecture:\n\n```\n└── ibm\n    └── gse\n        └── eda\n            └── inventory\n                ├── app\n                │   └── ItemAggregatorApplication.java\n                ├── domain\n                │   ├── ItemInventory.java\n                │   ├── ItemProcessingAgent.java\n                │   └── ItemTransaction.java\n                └── infra\n                    ├── ItemTransactionDeserializer.java\n                    ├── ItemTransactionStream.java\n                    └── api\n                        ├── InventoryResource.java\n                        ├── ItemCountQueries.java\n                        └── dto\n                            ├── ItemCountQueryResult.java\n                            └── PipelineMetadata.java\n```\n\nThe interesting class that supports the business logic is in [ItemProcessingAgent.java](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/main/java/ibm/gse/eda/inventory/domain/ItemProcessingAgent.java). \n\nBasically the logic to compute the different stocks are in the `processItemTransaction` method, which builds a Kafla Stream topology\n\nFor the stock of items cross store computation the code looks like:\n\n```java\n@Produces\npublic Topology processItemTransaction(){\n    KStream<String,ItemTransaction> items = inItemsAsStream.getItemStreams();     \n    // process items and aggregate at the store level \n    KTable<String,ItemInventory> itemItemInventory = items\n        // use store name as key, which is what the item event is also using\n        .map((k,transaction) -> {\n            ItemInventory newRecord = new ItemInventory();\n            newRecord.updateStockQuantityFromTransaction(transaction.sku, transaction);\n            return  new KeyValue<String,ItemInventory>(newRecord.itemID,newRecord);\n        })\n        .groupByKey( Grouped.with(Serdes.String(),ItemInventory.itemInventorySerde)).\n        aggregate( () -> new ItemInventory(),\n            (itemID,newValue,currentValue) -> currentValue.updateStockQuantity(itemID,newValue.currentStock),\n            materializeAsStoreInventoryKafkaStore());\n    produceStoreInventoryToOutputStream(itemItemInventory);\n    return inItemsAsStream.run();\n}\n```\n\nwhile for the store the code is also in [ItemProcessingAgent](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory/blob/main/src/main/java/ibm/gse/eda/stores/domain/ItemProcessingAgent.java)\n\n```java\npublic Topology processItemTransaction(){\n    KStream<String,ItemTransaction> items = inItemsAsStream.getItemStreams();     \n    // process items and aggregate at the store level \n    KTable<String,StoreInventory> storeItemInventory = items\n        // use store name as key, which is what the item event is also using\n        .groupByKey(ItemStream.buildGroupDefinitionType())\n        // update the current stock for this <store,item> pair\n        // change the value type\n        .aggregate(\n            () ->  new StoreInventory(), // initializer when there was no store in the table\n            (store , newItem, existingStoreInventory) \n                -> existingStoreInventory.updateStockQuantity(store,newItem), \n                materializeAsStoreInventoryKafkaStore());       \n    produceStoreInventoryToInventoryOutputStream(storeItemInventory);\n    return inItemsAsStream.run();\n\n```\n\nEach project includes a set of unit tests to validate the logic.\n\n## Integration tests\n\nFor running the integration test, we propose to copy the e2e folder from the solution repository and follow the [readme instructions section end-to-end-testing ](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing).\n\n## Deploy to OpenShift\n\nBe sure to have done [the steps described here](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to get user credentials and server side certificate. \n\nThe deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.  \nHere are the interesting properties to set environment variables from secrets \n\n```properties\n%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL\nquarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\nquarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12\nquarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password\nquarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12\nquarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.USER_CERT_PWD.value=user.password\n```\n\nAnd an extract of the expected generated openshift manifests from those configurations:\n\n```yaml\n    spec:\n      containers:\n      - env:\n        - name: KAFKA_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: ca.password\n              name: sandbox-rp-cluster-ca-cert\n        - name: USER_CERT_PATH\n          value: /deployments/certs/user/user.p12\n        - name: USER_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: KAFKA_BROKERS\n          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n        - name: KAFKA_CERT_PATH\n          value: /deployments/certs/server/ca.p12\n        - name: KAFKA_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: SECURE_PROTOCOL\n          value: SASL_SSL\n```\n\nFinally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:\n\n```\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred\n```\n\nwhich generates:\n\n```\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: es-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n```\n\n## Interactive queries\n\nWe already addressed the interactive queries concept in [the kafka stream technology summary article](/technology/kafka-streams/#interactive-queries). \nEach of the store and item aggregator implements those queries via two classes:\n\n* [ItemCountQueries](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/main/java/ibm/gse/eda/inventory/infra/api/ItemCountQueries.java)\n* [StoreInventoryQueries](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory/blob/main/src/main/java/ibm/gse/eda/stores/infra/api/StoreInventoryQueries.java)\n\nThe principles are the same:\n\n* Get the metadata about each \"kafka store\" supporting the stateful KTables which are keeping the aggregate per item or per store.\n* Get the value of the aggregate for the given key, locally or remotely.\n\n\n## Event Streams on Cloud as Kafka provider\n\nTo be completed.\n\n### Connect to Event Streams\n\nWe suppose you have created an Event Streams service instance on IBM Cloud. If not you can see some of [our getting started tutorial](/technology/event-streams/es-cloud/)\n\n* Connect to the cluster with `ibmcloud login -a ....` than init a session to your cluster: `ibmcloud es init`\n* Create the needed topics, following the instructions as described [in this note](../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:\n\n ```shell\n cloudctl es topic-create --name items --partitions 3 --replication-factor 3\n cloudctl es topic-create --name item.inventory --partitions 1 --replication-factor 3\n cloudctl es topic-create --name store.inventory --partitions 1 --replication-factor 3\n cloudctl es topics\n ```\n\n* To connect from the deployed application running on OpenShift to Event Streams on cloud we need to use different security settings:\n we need to define a user with `scram-sha-512` password, as this is the mechanism for external to the cluster connection. [See product documentation](https://ibm.github.io/event-streams/getting-started/connecting/) on how to do it, or use our [quick summary here](/use-cases/overview/pre-requisites#get-shram-user).\n\n* Get Server TLS certificate into the `certs` folder. See our [quick summary here](/use-cases/overview/pre-requisites#get-tls-server-public-certificate)\n\n ```shell\n ```\n\n* Modify the `application.properties` file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka streams can create topics to backup state stores, and one for kafka streams consumer and producer tasks:\n\n```properties\nkafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\n%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\n%dev.kafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=${USER_CERT_PATH}\n%prod.kafka.ssl.keystore.password=${USER_CERT_PWD}\n%prod.kafka.ssl.keystore.type=PKCS12\n```\n\nThe above settings take into account that when running locally (`%dev` profile) we use the `scram-sha` mechanism to authenticate, and when we deploy on openshift, the `%prod` profile is used with TLS mutual authentication  (client certificate in keystore).\n\nThe same approach applies for Kafka Stream:\n\n```\nquarkus.kafka-streams.bootstrap-servers=${KAFKA_BROKERS}\nquarkus.kafka-streams.security.protocol=${SECURE_PROTOCOL}\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\n%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\n%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\nquarkus.kafka-streams.ssl.truststore.location=${KAFKA_CERT_PATH}\nquarkus.kafka-streams.ssl.truststore.password=${KAFKA_CERT_PWD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n# Only if TLS is used for authentication instead of scram\n%prod.quarkus.kafka-streams.ssl.keystore.location=${USER_CERT_PATH}\n%prod.quarkus.kafka-streams.ssl.keystore.password=${USER_CERT_PWD}\n%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12\n```\n\n* Define a file, like `.env`, to set environment variables, and modify the settings from your Event Streams configuration.\n\n```\nKAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\nKAFKA_USER=\nKAFKA_PASSWORD=\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=\nSECURE_PROTOCOL=SASL_SSL\n```\n","type":"Mdx","contentDigest":"153c9cb9232a148b73ec1c22be711a70","owner":"gatsby-plugin-mdx","counter":910},"frontmatter":{"title":"Kafka Streams Test Lab 3","description":"Using Kafka Streams to compute real time inventory stock"},"exports":{},"rawBody":"---\ntitle: Kafka Streams Test Lab 3\ndescription: Using Kafka Streams to compute real time inventory stock\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Work in progress</strong> Updated 06/09/2021\n</InlineNotification>\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Pre-requisites</AnchorLink>\n    <AnchorLink>One Click Deploy to OpenShift</AnchorLink>\n    <AnchorLink>Testing the solution</AnchorLink>\n    <AnchorLink>Understanding the Kafka Streams implementation</AnchorLink>\n    <AnchorLink>Interactive queries</AnchorLink>\n    <AnchorLink>Event Streams on Cloud as Kafka provider</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n\nIn this lab, we're going to use [Quarkus](https://quarkus.io) to develop the near real-time inventory logic using Kafka Streams APIs and microprofile reactive messaging.\n\nThe requirements to address are:\n\n- consume item sold events from the `items` topic. Item has SKU as unique key. Item event has store ID reference\n- the Kafka record in the `items` topic, uses the Store unique ID as key\n- compute for each item its current stock cross stores\n- compute the store's stock for each item\n- generate inventory event for store - item - stock\n- expose APIs to get stock for a store or for an item\n\n![0](./images/inventory-use-cases.png)\n\nThe solution is using Kafka Streams and it includes two services. The components used are:\n\n ![1](./images/inventory-components.png)\n\nThe goal of this lab, is to develop the green components which expose APIs to support Kafka Streams interactive query on top of the aggregates to keep\nthe inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic).\n\nWe will be unit testing the stream logic using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TopologyTestDriver class. \n\nThis solution is deployed to OpenShift cluster with Strimzi running in the same cluster and namespace.\n\nThis application needs the [Item Store sell simulator](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) to perform the end to end \ntesting and to demonstrate the end to end scenario.\n\n## Pre-requisites\n\n* Access to an **OpenShift Container Platform** v4.6.x\n* **Kafka**: The lab can use Event Streams on Cloud or Kafka Strimzi deployed on OpenShift.\n* **Code Source**: from the git repositories: \n  \n    * [https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory) to compute item inventory cross store\n    * [https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory) to compute store inventory.\n    * [https://github.com/ibm-cloud-architecture/eda-lab-inventory](https://github.com/ibm-cloud-architecture/eda-lab-inventory) to get access to deployment configuration to deploy on OpenShift.\n\n  ```sh \n  git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory\n  git clone https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory\n  git clone https://github.com/ibm-cloud-architecture/eda-lab-inventory\n  ```\n* OC CLI\n* As in previous labs, be connected to your Openshift cluster.\n\nWe have automated the deployment of all the pieces making up this use case. We are using the open source Strimzi operator to get a Kafka cluster deployed\n\n### IBM OpenLabs\n\nIn this section, we are going to see use the [IBM OpenLabs](https://developer.ibm.com/openlabs/openshift) hosted environment.\n\n1. Go to [IBM OpenLabs](https://developer.ibm.com/openlabs/openshift) in a browser and click on `Launch Lab` button for **Bring Your Own Application**.\n\n  ![OpenLabs1](images/openLabs1.png)\n\n1. Sign in with your IBM Cloud account or register for an IBM Cloud account.\n\n  ![OpenLabs2](images/openLabs2.png)\n\n1. You will be presented with a dialog asking you whether you have an **Opportunity Id** or not. \nIf you don't have it or don't no, just select **No** and click on **Launch Lab**.\n\n1. You should now see your IBM OpenLabs environment.\n\n  ![OpenLabs4](images/openLabs4.png)\n\n1. On the left hand side navigation menu, click on the **Quick Links and Common Commands** section. \nNow, if you scroll down on the instructions shown on your screen, you should reach the **Commonly Used Commands** \nsection of these and in there you should see an `oc login ...` command to get your terminal associated to this\n IBM OpenLabs logged into the OpenShift cluster that you will be working with for this quickstart tutorial. \nClick on the `oc login...` command and you should see a `Login successful` message on the terminal.\n\n  ![OpenLabs8](images/openLabs8.png)\n\n\n## One Click Deploy to OpenShift\n\nThe different components are deployed in the same namespace as the Kafka cluster, and use internal route to access Kafka bootstrap URL.\n\nThe images for each of the components used are in the [quay.io ibmcase](https://quay.io/organization/ibmcase) repository:\n\n![](./images/quay-images.png)\n\n1. Under the `eda-lab-inventory` folder open the `scripts/env-strimzi.sh` file to provide your own values or use our defined values:\n  \n  * **KAFKA_CLUSTER_NAME** which is the name you want to give to the Kafka cluster that will get deployed. It defaults to my-kafka and we highly recommend you to leave the default. Otherwise, you would need to modify some yaml deployment scripts…\n  * **YOUR_PROJECT_NAME** which is the name of the OpenShift project the one-click-deployment script will create for you to get all the components of this solution deployed into. It defaults to rt-inventory. \n\n1. Execute the `deployInventoryWithStrimzi.sh` one-click-deployment script:\n\n   ```sh\n   ./scripts/deployInventoryWithStrimzi.sh --skip-login\n   ```\n\n  This script can be run multiple times, in case of failure, it should continue creating the needed resources and apps.\n\n1. After some time (can easily take up to more than 10 min) you should see the following message:\n\n\n  ```sh\n  ********************\n  ** CONGRATULATIONS!! You have successfully deployed the realtime inventory use case. \n  ********************\n  ```\n1. Verify the running pods\n\n  ```sh\n  NAME                                        READY   STATUS    RESTARTS   AGE\n  item-aggregator-557db4c9c6-xds24            1/1     Running   0          8m47s\n  my-kafka-cruise-control-69dd479bf6-bmnt6    2/2     Running   0          10m\n  my-kafka-entity-operator-7d5f948d84-9knnn   3/3     Running   0          11m\n  my-kafka-kafka-0                            1/1     Running   0          12m\n  my-kafka-kafka-1                            1/1     Running   0          12m\n  my-kafka-kafka-2                            1/1     Running   0          12m\n  my-kafka-zookeeper-0                        1/1     Running   0          13m\n  my-kafka-zookeeper-1                        1/1     Running   0          13m\n  my-kafka-zookeeper-2                        1/1     Running   0          12m\n  store-aggregator-7df98556ff-rdhcw           1/1     Running   0          4m59s\n  store-simulator-56f8958498-44q9q            1/1     Running   0          9m30s\n  ```\n\n1. Get the different service endpoints:\n\n  ```sh\n  oc get routes\n\n  item-aggregator            item-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud            \n  my-kafka-kafka-bootstrap   my-kafka-kafka-bootstrap-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud   \n  store-aggregator           store-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud           \n  store-simulator            store-simulator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud \n  ```\n\n1. You can also access the OpenShift Console and go to the installed Operators and select Strimzi. In this operators you can use the cluster, the users and the topics:\n\n  ![](./images/strimzi-topics.png)\n\n### Some explanations of this deployment script\n\nThe script performs the following steps:\n\n* Create a project if it does not exist\n* Create a service account for the solution and a role binding to cluster role views\n* Create Strimzi operator if it is not already there\n* Create a kafka cluster in the project with 3 brokers and zookeeper\n* Create scram and tls user\n* Create the topics for solution\n* Create one config map to define topics and broker boostrap shareable between apps\n* Deploy each microservices\n\n## Testing the solution\n\nWe have moved the demonstration script to the [scenario chapter](/scenarios/realtime-inventory/#demonstration-script-for-the-solution).\n\n\n## Understanding the Kafka Streams implementation\n\nThe item and store aggregator code are based on the same code structure, reflecting the DDD onion architecture:\n\n```\n└── ibm\n    └── gse\n        └── eda\n            └── inventory\n                ├── app\n                │   └── ItemAggregatorApplication.java\n                ├── domain\n                │   ├── ItemInventory.java\n                │   ├── ItemProcessingAgent.java\n                │   └── ItemTransaction.java\n                └── infra\n                    ├── ItemTransactionDeserializer.java\n                    ├── ItemTransactionStream.java\n                    └── api\n                        ├── InventoryResource.java\n                        ├── ItemCountQueries.java\n                        └── dto\n                            ├── ItemCountQueryResult.java\n                            └── PipelineMetadata.java\n```\n\nThe interesting class that supports the business logic is in [ItemProcessingAgent.java](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/main/java/ibm/gse/eda/inventory/domain/ItemProcessingAgent.java). \n\nBasically the logic to compute the different stocks are in the `processItemTransaction` method, which builds a Kafla Stream topology\n\nFor the stock of items cross store computation the code looks like:\n\n```java\n@Produces\npublic Topology processItemTransaction(){\n    KStream<String,ItemTransaction> items = inItemsAsStream.getItemStreams();     \n    // process items and aggregate at the store level \n    KTable<String,ItemInventory> itemItemInventory = items\n        // use store name as key, which is what the item event is also using\n        .map((k,transaction) -> {\n            ItemInventory newRecord = new ItemInventory();\n            newRecord.updateStockQuantityFromTransaction(transaction.sku, transaction);\n            return  new KeyValue<String,ItemInventory>(newRecord.itemID,newRecord);\n        })\n        .groupByKey( Grouped.with(Serdes.String(),ItemInventory.itemInventorySerde)).\n        aggregate( () -> new ItemInventory(),\n            (itemID,newValue,currentValue) -> currentValue.updateStockQuantity(itemID,newValue.currentStock),\n            materializeAsStoreInventoryKafkaStore());\n    produceStoreInventoryToOutputStream(itemItemInventory);\n    return inItemsAsStream.run();\n}\n```\n\nwhile for the store the code is also in [ItemProcessingAgent](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory/blob/main/src/main/java/ibm/gse/eda/stores/domain/ItemProcessingAgent.java)\n\n```java\npublic Topology processItemTransaction(){\n    KStream<String,ItemTransaction> items = inItemsAsStream.getItemStreams();     \n    // process items and aggregate at the store level \n    KTable<String,StoreInventory> storeItemInventory = items\n        // use store name as key, which is what the item event is also using\n        .groupByKey(ItemStream.buildGroupDefinitionType())\n        // update the current stock for this <store,item> pair\n        // change the value type\n        .aggregate(\n            () ->  new StoreInventory(), // initializer when there was no store in the table\n            (store , newItem, existingStoreInventory) \n                -> existingStoreInventory.updateStockQuantity(store,newItem), \n                materializeAsStoreInventoryKafkaStore());       \n    produceStoreInventoryToInventoryOutputStream(storeItemInventory);\n    return inItemsAsStream.run();\n\n```\n\nEach project includes a set of unit tests to validate the logic.\n\n## Integration tests\n\nFor running the integration test, we propose to copy the e2e folder from the solution repository and follow the [readme instructions section end-to-end-testing ](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing).\n\n## Deploy to OpenShift\n\nBe sure to have done [the steps described here](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to get user credentials and server side certificate. \n\nThe deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.  \nHere are the interesting properties to set environment variables from secrets \n\n```properties\n%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL\nquarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\nquarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12\nquarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password\nquarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12\nquarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.USER_CERT_PWD.value=user.password\n```\n\nAnd an extract of the expected generated openshift manifests from those configurations:\n\n```yaml\n    spec:\n      containers:\n      - env:\n        - name: KAFKA_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: ca.password\n              name: sandbox-rp-cluster-ca-cert\n        - name: USER_CERT_PATH\n          value: /deployments/certs/user/user.p12\n        - name: USER_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: KAFKA_BROKERS\n          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n        - name: KAFKA_CERT_PATH\n          value: /deployments/certs/server/ca.p12\n        - name: KAFKA_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: SECURE_PROTOCOL\n          value: SASL_SSL\n```\n\nFinally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:\n\n```\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred\n```\n\nwhich generates:\n\n```\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: es-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n```\n\n## Interactive queries\n\nWe already addressed the interactive queries concept in [the kafka stream technology summary article](/technology/kafka-streams/#interactive-queries). \nEach of the store and item aggregator implements those queries via two classes:\n\n* [ItemCountQueries](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/main/java/ibm/gse/eda/inventory/infra/api/ItemCountQueries.java)\n* [StoreInventoryQueries](https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory/blob/main/src/main/java/ibm/gse/eda/stores/infra/api/StoreInventoryQueries.java)\n\nThe principles are the same:\n\n* Get the metadata about each \"kafka store\" supporting the stateful KTables which are keeping the aggregate per item or per store.\n* Get the value of the aggregate for the given key, locally or remotely.\n\n\n## Event Streams on Cloud as Kafka provider\n\nTo be completed.\n\n### Connect to Event Streams\n\nWe suppose you have created an Event Streams service instance on IBM Cloud. If not you can see some of [our getting started tutorial](/technology/event-streams/es-cloud/)\n\n* Connect to the cluster with `ibmcloud login -a ....` than init a session to your cluster: `ibmcloud es init`\n* Create the needed topics, following the instructions as described [in this note](../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:\n\n ```shell\n cloudctl es topic-create --name items --partitions 3 --replication-factor 3\n cloudctl es topic-create --name item.inventory --partitions 1 --replication-factor 3\n cloudctl es topic-create --name store.inventory --partitions 1 --replication-factor 3\n cloudctl es topics\n ```\n\n* To connect from the deployed application running on OpenShift to Event Streams on cloud we need to use different security settings:\n we need to define a user with `scram-sha-512` password, as this is the mechanism for external to the cluster connection. [See product documentation](https://ibm.github.io/event-streams/getting-started/connecting/) on how to do it, or use our [quick summary here](/use-cases/overview/pre-requisites#get-shram-user).\n\n* Get Server TLS certificate into the `certs` folder. See our [quick summary here](/use-cases/overview/pre-requisites#get-tls-server-public-certificate)\n\n ```shell\n ```\n\n* Modify the `application.properties` file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka streams can create topics to backup state stores, and one for kafka streams consumer and producer tasks:\n\n```properties\nkafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\n%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\n%dev.kafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=${USER_CERT_PATH}\n%prod.kafka.ssl.keystore.password=${USER_CERT_PWD}\n%prod.kafka.ssl.keystore.type=PKCS12\n```\n\nThe above settings take into account that when running locally (`%dev` profile) we use the `scram-sha` mechanism to authenticate, and when we deploy on openshift, the `%prod` profile is used with TLS mutual authentication  (client certificate in keystore).\n\nThe same approach applies for Kafka Stream:\n\n```\nquarkus.kafka-streams.bootstrap-servers=${KAFKA_BROKERS}\nquarkus.kafka-streams.security.protocol=${SECURE_PROTOCOL}\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\n%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\n%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\nquarkus.kafka-streams.ssl.truststore.location=${KAFKA_CERT_PATH}\nquarkus.kafka-streams.ssl.truststore.password=${KAFKA_CERT_PWD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n# Only if TLS is used for authentication instead of scram\n%prod.quarkus.kafka-streams.ssl.keystore.location=${USER_CERT_PATH}\n%prod.quarkus.kafka-streams.ssl.keystore.password=${USER_CERT_PWD}\n%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12\n```\n\n* Define a file, like `.env`, to set environment variables, and modify the settings from your Event Streams configuration.\n\n```\nKAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\nKAFKA_USER=\nKAFKA_PASSWORD=\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=\nSECURE_PROTOCOL=SASL_SSL\n```\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-streams/lab-3/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}