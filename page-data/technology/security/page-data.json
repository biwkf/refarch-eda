{"componentChunkName":"component---src-pages-technology-security-index-mdx","path":"/technology/security/","result":{"pageContext":{"frontmatter":{"title":"Kafka Security Overview","description":"Kafka Security Overview"},"relativePagePath":"/technology/security/index.mdx","titleType":"append","MdxNode":{"id":"69a6eeb5-a4a6-5b53-9590-d8db67b54135","children":[],"parent":"087b9bc6-13bf-570b-bb7c-4d3c8defba15","internal":{"content":"---\ntitle: Kafka Security Overview\ndescription: Kafka Security Overview\n---\n\nUpdated 04/13/2022\n\nReview [this video for a refresh on SSL and TLS certificates](https://www.youtube.com/watch?v=T4Df5_cojAs) and keep in mind what the speaker quotes:\n\n> * Any message encrypted with Bob's public key can only be decrypted with Bob's private key\n> * Anyone with access to Alice's public key can verify that a message could only have been created by someone with access to Alice's private key.\n\n![](./images/tls-overview.png)\n\nFor a deeper dive into security administration see [this](https://docs.confluent.io/platform/current/security/general-overview.html) confluent article and [Kafka's product documentation](http://kafka.apache.org/documentation/#security).\n\nWe also **strongly recommend** reading Rick Osowski's blogs [Part 1](https://rosowski.medium.com/kafka-security-fundamentals-the-rosetta-stone-to-your-event-streaming-infrastructure-518f49640db4) and [Part 2](https://rosowski.medium.com/kafka-security-fundamentals-adding-tls-to-your-event-driven-utility-belt-432307f4ff62) on Kafka security configuration.\n\n## Understand the Kafka cluster listeners\n\nYou can secure your IBM Event Streams resources by managing the access each user and application has to each resource.\n\nAn Event Streams cluster can be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers and these can be configured as secured listeners (which is the default for the `tls` and `external` Kafka listener you will see below).\n\nEach Kafka listener providing a connection to Event Streams can also be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms. \nAdditionally, the Event Streams cluster can be configured to authorize operations sent via an authenticated listener using access control list defined at the user level.\n\nThe following figure presents a decision tree and the actions to consider for configuring cluster and applications.\n\n![](./images/decision-security.png)\n\nIn Event Streams, the following yaml snippet from an IBM Event Streams instance definition defines the following Kafka listeners\"\n\n* One internal non secured kafka listener on port `9092` called `plain`\n* One internal secured (TLS encrypted) Kafka listener on port `9093` called `tls`, which also enforces authentication throughout TLS, and \n* One external secured (TLS encrypted) Kafka listener on port `9094` called `external`, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.\n\n    ```yaml\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n        authentication:\n            type: tls\n      - name: external\n        type: route\n        port: 9094\n        tls: true \n        authentication:\n          type: scram-sha-512\n    ```\n    (\\*) `tls: true` enforces traffic encryption. Default is true for Kafka listeners on ports `9093` and `9094`\n\n    (\\**) `type: internal` specifies that a Kafka listener is internal. Kafka listenes on ports `9092` and `9093` default to internal.\n\n\n## To connect to kafka using Kafka API\n\nThe most important and essential property to connect to a Kafka broker is the `bootstrap.servers` property. This property tells Kafka clients what URL to use to talk to kafka cluster. `bootstrap.server` defines what Kafka listener your application will use to connect to Kafka. And based on that Kafka listener, you may need to provide your application with extra configuration.\n\nAt the very minimum, you will need to set the [security.protocol](http://kafka.apache.org/documentation/#adminclientconfigs_security.protocol) property that will tell whether you are connecting to a secured Kafka listener or not. As a result, the values for `security.protocol` are:\n\n  * `PLAINTEXT` - using PLAINTEXT transport layer & no authentication - default value.\n  * `SSL` - using SSL transport layer & certificate-based authentication or no authentication.\n  * `SASL_PLAINTEXT` - using PLAINTEXT transport layer & SASL-based authentication.\n  * `SASL_SSL` - using SSL transport layer & SASL-based authentication.\n\nBased on the above, the security protocol you will use to connect to the different Kafka listeners that IBM Event Streams deploys are:\n\n   - `PLAINTEXT` when connecting to the non secured internal `plain` Kafka listener on port `9092`\n   - `SSL` when connecting to the secured (TLS encrypted) internal `tls` Kafka listener on port `9093` that also enforces authentication through TLS certificates\n   - `SASL_SSL` when connecting to the secured (TLS encrypted) external `external` Kafka listener on port `9094` that also enforces authentication through SCRAM credentials.\n\n### Non-secured listener\n\nYou would only need to specify that there is no security in place for your application to connect to a non-secured kafka listener:\n\n```properties\nsecurity.protocol=PLAINTEXT\n```\n\n### Secured listener\n\nIn order for your application to be able to connect to Kafka through the internal secured (TLS encrypted) Kafka listener, you need to set the appropriate value for `security.protocol` as seen above plus provide the Certificate Authority of the Kafka cluster (its public key).\n\nDepending on the technology of your application, you will need to provide the Certificate Authority of the Kafka cluster for the TLS encryption either as a `PKCS12` certificate for a Java client or as a `PEM` certificate for anything else. `PKCS12` certificates (or truststores) come in the form of a `.p12` file and are secured with a password. You can inspect a `PKCS12` certificate with:\n\n```sh\nopenssl pkcs12 -info -nodes -in truststore.p12\n```\n\nand providing the truststore password.\n\nAn example of the output would be:\n\n```sh\nMAC Iteration 100000\nMAC verified OK\nPKCS7 Encrypted data: Certificate bag\nBag Attributes\n    friendlyName: ca.crt\n    2.16.840.1.113894.746875.1.1: <Unsupported tag 6>\nsubject=/O=io.strimzi/CN=cluster-ca v0\nissuer=/O=io.strimzi/CN=cluster-ca v0\n-----BEGIN CERTIFICATE-----\nMIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL\nBQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2\nMDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv\nLnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB\nAQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx\nOF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j\ndM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c\nBj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk\nH2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD\n25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud\nDgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8\nRkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa\n/aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq\n3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag\njxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0\nJsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n```\n\nOn the other hand, `PEM` certificates come in the form of a `.pem` file and are not password protected.\n\nYou can inspect them using `cat`. \n\nThe output should be the same certificate as the one provided within the `PKCS12` certificate:\n\n```sh\ncat es-cert.pem \n-----BEGIN CERTIFICATE-----\nMIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL\nBQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2\nMDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv\nLnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB\nAQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx\nOF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j\ndM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c\nBj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk\nH2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD\n25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud\nDgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8\nRkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa\n/aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq\n3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag\njxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0\nJsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n```\n\nYou can find in the [Event Streams with CP4i](../event-streams/es-cp4i/#get-event-streams-tls-certificates) section how to obtain the Certificate Authority of your IBM Event Streams instance.\n\nOnce you have the Certificate Authority of your Kafka cluster, you will provide its location and password in your properties file through the `ssl.truststore.location` and `ssl.truststore.password` properties.\n\n\n```properties\nsecurity.protocol=SSL or SASL_SSL\nssl.protocol=TLSv1.2\n\nssl.truststore.password=<truststore.p12-password>\nssl.truststore.location=truststore.p12\nssl.truststore.type=PKCS12\n```\n\nwhere `security.protocol` will vary between `SSL` or `SASL_SSL` based on the authentication as you will see next.\n\n### Authentication\n\nYou have seen above that your Kafka listeners can require authentication to any application or client wanting to connect to the Kafka cluster through them. It was also said that authentication could be either of type SASL-based, through SCRAM (modern Salted Challenge Response Authentication Mechanism) credentials, or certificate-based (TLS). Either way, IBM Event Streams will handle authentication through `KafkaUser` objects. \n\nThese objects that represent Kafka users of your IBM Event Streams instance will have their authentication (and authorization through ACLs) credentials or TLS certificates associated to them stored in a secret. In order to find out how to create these `KafkaUsers`, which will vary depending on the authentication method, check out [this section](../event-streams/es-cp4i#generate-scram-service-credentials).\n\n#### Scram\n\nIf you have created a `KafkaUser` to be used with a Kafka listener that requires SCRAM authentication, you will be able to retrieve its SCRAM credentials either from the IBM Event Streams UI at creation time or later on from the secret these are stored to:\n\n```sh\noc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys=sasl.jaas.config --to=-\n```\n\nwhere\n\n- `<KAFKA_USER>` is the name of the `KafkaUser` object you created.\n- `<NAMESPACE>` is the namespace where IBM Event Streams is deployed on.\n\n  Example:\n\n  ```sh\n  oc extract secret/test-app -n tools --keys=sasl.jaas.config --to=-\n  # sasl.jaas.config\n  org.apache.kafka.common.security.scram.ScramLoginModule required username=\"test-app\" password=\"VgWpkjAkvxH0\";\n  ```\n\n  You can see above your SCRAM username and password.\n\n#### TLS\n\nIf you have created a `KafkaUser` to be used with a Kafka listener that requires TLS authentication, \nyou will be able to retrieve its TLS certificates either from the IBM Event Streams UI at creation time in a zip folder or \nlater on from the secret these are stored to.\n\nFirst, describe the secret to see what certificates are stored in it:\n\n```sh\n$ oc describe secret test-app-tls -n tools\nName:         test-app-tls\nNamespace:    tools\nLabels:       app.kubernetes.io/instance=test-app-tls\n              app.kubernetes.io/managed-by=strimzi-user-operator\n              app.kubernetes.io/name=strimzi-user-operator\n              app.kubernetes.io/part-of=eventstreams-test-app-tls\n              eventstreams.ibm.com/cluster=es-inst\n              eventstreams.ibm.com/kind=KafkaUser\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\nuser.key:       1704 bytes\nuser.p12:       2384 bytes\nuser.password:  12 bytes\nca.crt:         1180 bytes\nuser.crt:       1025 bytes\n```\n\nYou can see that the secret will store the following:\n\n- `user.key` and `user.crt` - the client certificate key-pair.\n- `user.p12` - trustore that contains the `user.key` and `user.crt`.\n- `user.password` - contains the `user.p12` truststore password.\n- `ca.crt` - CA used to sign the client certificate key-pair.\n\nThen, you can extract the appropriate certificate based on whether your application or Kafka client is Java based or not. In the case of a Java based application or Kafka client, extract the `user.p12` and `user.password`:\n\n```sh\noc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys=user.p12\noc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys=user.password\n```\n\nwhere\n\n- `<KAFKA_USER>` is the name of the `KafkaUser` object you created.\n- `<NAMESPACE>` is the namespace where IBM Event Streams is deployed on.\n\n#### Properties config\n\nNow that you know how to get the authentication credentials or certificates for a proper authentication of your application or Kafka client you need to configure the appropriate properties for that:\n\n- If your Kafka listener authentication method is SCRAM:\n\n  ```properties\n  security.protocol=SASL_SSL\n\n  sasl.mechanism=SCRAM-SHA-512\n  sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<USERNAME>\" password\\=\"<PASSWORD>\";\n  ```\n\n- If your Kafka listener authentication method is TLS:\n\n  ```properties\n  security.protocol=SSL\n\n  ssl.keystore.location=<location_to_your_user.p12>\n  ssl.keystore.password=<user.p12-password>\n  ssl.keystore.type=PKCS12\n  ```\n\n## Recapitulation\n\nLet's have a full look at how the Kafka communication properties, for a Java application or client, would look like for IBM Event Streams \non RedHat OpenShift with the defaults. If you look at the IBM Event Streams instance deployment sample definitions \nin [this](https://github.com/ibm-messaging/event-streams-operator-resources/tree/master/cr-examples/eventstreams) GitHub repository, \nthat is mentioned in the IBM Event Streams official documentation [here](https://ibm.github.io/event-streams/installing/configuring/), \nyou will see that the defaults defined for the Kafka listeners for all of them (except from the `light-insecure.yaml` sample) are:\n\n```yaml\nlisteners:\n  external:\n    type: route\n    authentication:\n      type: scram-sha-512\n  tls:\n    authentication:\n      type: tls\n```\n\nThis translates to Strimzi (the open source project IBM Event Streams is based on) in:\n\n```yaml\nlisteners:\n  - name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n        type: tls\n  - name: external\n    type: route\n    port: 9094\n    tls: true \n    authentication:\n      type: scram-sha-512\n```\n\nLet's also add the `plain` non-secure Kafka listener to the picture so that all cases are covered in this recap section.\n\n```yaml\nlisteners:\n  plain: {}\n  external:\n    type: route\n    authentication:\n      type: scram-sha-512\n  tls:\n    authentication:\n      type: tls\n```\n\nAs a result, the IBM Event Streams instance deployed will count with:\n\n* One internal non secured kafka listener on port `9092` called `plain`\n* One internal secured (TLS encrypted) Kafka listener on port `9093` called `tls`, which also enforces authentication throughout TLS, and \n* One external secured (TLS encrypted) Kafka listener on port `9094` called `external`, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.\n\n### Plain\n\nThe Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the non secured kafka listener on port `9092` called `plain` will be as follows:\n\n```properties\n# Internal plain listener\n# =======================\nsecurity.protocol=PLAINTEXT\nbootstrap.servers=<ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9092\n```\n\nwhere\n\n- `<ES_NAME>` is the name of the IBM Event Streams instance deployed you are trying to connect to.\n- `<NAMESPACE>` is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.\n\n### Internal tls\n\nThe Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the internal s\necured (TLS encrypted) Kafka listener on port `9093` called `tls`, which also enforces authentication throughout mTLS will be as follows:\n\n```properties\n# Internal tls listener\n# =====================\nbootstrap.servers=<<ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9093\n\nsecurity.protocol=SSL\nssl.protocol=TLSv1.2\n\n## mTLS Authentication for the client.\nssl.keystore.location=<user.p12-location>\nssl.keystore.password=<user.p12-password>\nssl.keystore.type=PKCS12\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=<trustore.p12-password>\nssl.truststore.location=<truststore.p12-location>\nssl.truststore.type=PKCS12\n```\n\nwhere\n\n- `<ES_NAME>` is the name of the IBM Event Streams instance deployed you are trying to connect to.\n- `<NAMESPACE>` is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.\n- `<user.p12-location>` is the location of the `user.p12` truststore containing the `user.key` and `user.crt` client certificate key-pair for the application or client mTLS authentication as explained above.\n- `<user.p12-password>` is the password of the `<user.p12>` truststore.\n- `<truststore.p12-location>` is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client.\n- `<trustore.p12-password>` is the password for the `truststore.p12` truststore.\n\nWhen the application is deployed on OpenShift, certificates will be mounted to the application pod. Below is an example of a Quarkus app deployment \ndescriptor, with environment variables:\n\n```yaml\nenv:\n  - name: KAFKA_SSL_TRUSTSTORE_FILE_LOCATION\n  value: /deployments/certs/server/ca.p12\n- name: KAFKA_SSL_TRUSTSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SSL_KEYSTORE_FILE_LOCATION\n  value: /deployments/certs/user/user.p12\n- name: KAFKA_SSL_KEYSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SECURITY_PROTOCOL\n  value: SSL\n- name: KAFKA_USER\n  value: tls-user\n- name: KAFKA_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: ca.password\n      name: kafka-cluster-ca-cert\n- name: USER_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: user.password\n      name: tls-user\n# ...\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: kafka-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n      volumes:\n      - name: kafka-cert\n        secret:\n          optional: true\n          secretName: kafka-cluster-ca-cert\n      - name: user-cert\n        secret:\n          optional: true\n          secretName: tls-user\n```\n\n### External tls\n\nThe Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the external secured (TLS encrypted) Kafka listener on port `9094` called `external`, which also enforces authentication throughout SCRAM credentials, and that is exposed through a route will be as follows:\n\n```properties\n# External listener SCRAM\n# =======================\nbootstrap.servers=<ES_NAME>-kafka-bootstrap-<NAMESPACE>.<OPENSHIFT_APPS_DNS>\\:443\n\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=<trustore.p12-password>\nssl.truststore.location=<truststore.p12-location>\nssl.truststore.type=PKCS12\n\n## Scram credentials\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<SCRAM_USERNAME>\" password\\=\"<SCRAM_PASSWORD>\";\n```\n\nwhere\n\n- `<ES_NAME>` is the name of the IBM Event Streams instance deployed you are trying to connect to.\n- `<NAMESPACE>` is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.\n-  `<OPENSHIFT_APPS_DNS>` is your RedHat OpenShift DNS domain for application routes.\n- `<truststore.p12-location>` is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client.\n- `<trustore.p12-password>` is the password for the `truststore.p12` truststore.\n- `<SCRAM_USERNAME>` and `<SCRAM_PASSWORD>` are your SCRAM credentials.\n\n## Tips\n\nRemember that if the application does not run in the same namespace as the kafka cluster then you need to copy the secrets so that \nthe application developers can access the required credentials and certificates from their own namespaces with something like\n\n```sh\nif [[ -z $(oc get secret ${TLS_USER} 2> /dev/null) ]]\nthen\n   # As the project is personal to the user, we can keep a generic name for the secret\n   oc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2> /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n```\n\n## Kafka Connect\n\nFor Kafka connector, you need to define authentication used to connect to the Kafka Cluster:\n\n```yaml\n  authentication: \n    type: tls\n    certificateAndKey:\n      secretName: tls-user\n      certificate: user.crt\n      key: user.key\n```\n\n\n* Get TLS public cluster certificate:\n\n```yaml\n  tls: \n    trustedCertificates:\n      - secretName: dev-cluster-ca-cert\n        certificate: ca.crt\n```\n\n## Working with certificates\n\nTo extract a PEM-based certificate from a JKS-based truststore, you can use the following command:\n\n```sh\nkeytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n```\n\nTo build a PKCS12 from a pem do\n\n```sh\nopenssl pkcs12 -export -in cert.pem -out cert.p12\n# if you want jks\nkeytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks\n```","type":"Mdx","contentDigest":"1fb00f7ca2edc4206ada877d7db46348","owner":"gatsby-plugin-mdx","counter":910},"frontmatter":{"title":"Kafka Security Overview","description":"Kafka Security Overview"},"exports":{},"rawBody":"---\ntitle: Kafka Security Overview\ndescription: Kafka Security Overview\n---\n\nUpdated 04/13/2022\n\nReview [this video for a refresh on SSL and TLS certificates](https://www.youtube.com/watch?v=T4Df5_cojAs) and keep in mind what the speaker quotes:\n\n> * Any message encrypted with Bob's public key can only be decrypted with Bob's private key\n> * Anyone with access to Alice's public key can verify that a message could only have been created by someone with access to Alice's private key.\n\n![](./images/tls-overview.png)\n\nFor a deeper dive into security administration see [this](https://docs.confluent.io/platform/current/security/general-overview.html) confluent article and [Kafka's product documentation](http://kafka.apache.org/documentation/#security).\n\nWe also **strongly recommend** reading Rick Osowski's blogs [Part 1](https://rosowski.medium.com/kafka-security-fundamentals-the-rosetta-stone-to-your-event-streaming-infrastructure-518f49640db4) and [Part 2](https://rosowski.medium.com/kafka-security-fundamentals-adding-tls-to-your-event-driven-utility-belt-432307f4ff62) on Kafka security configuration.\n\n## Understand the Kafka cluster listeners\n\nYou can secure your IBM Event Streams resources by managing the access each user and application has to each resource.\n\nAn Event Streams cluster can be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers and these can be configured as secured listeners (which is the default for the `tls` and `external` Kafka listener you will see below).\n\nEach Kafka listener providing a connection to Event Streams can also be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms. \nAdditionally, the Event Streams cluster can be configured to authorize operations sent via an authenticated listener using access control list defined at the user level.\n\nThe following figure presents a decision tree and the actions to consider for configuring cluster and applications.\n\n![](./images/decision-security.png)\n\nIn Event Streams, the following yaml snippet from an IBM Event Streams instance definition defines the following Kafka listeners\"\n\n* One internal non secured kafka listener on port `9092` called `plain`\n* One internal secured (TLS encrypted) Kafka listener on port `9093` called `tls`, which also enforces authentication throughout TLS, and \n* One external secured (TLS encrypted) Kafka listener on port `9094` called `external`, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.\n\n    ```yaml\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n        authentication:\n            type: tls\n      - name: external\n        type: route\n        port: 9094\n        tls: true \n        authentication:\n          type: scram-sha-512\n    ```\n    (\\*) `tls: true` enforces traffic encryption. Default is true for Kafka listeners on ports `9093` and `9094`\n\n    (\\**) `type: internal` specifies that a Kafka listener is internal. Kafka listenes on ports `9092` and `9093` default to internal.\n\n\n## To connect to kafka using Kafka API\n\nThe most important and essential property to connect to a Kafka broker is the `bootstrap.servers` property. This property tells Kafka clients what URL to use to talk to kafka cluster. `bootstrap.server` defines what Kafka listener your application will use to connect to Kafka. And based on that Kafka listener, you may need to provide your application with extra configuration.\n\nAt the very minimum, you will need to set the [security.protocol](http://kafka.apache.org/documentation/#adminclientconfigs_security.protocol) property that will tell whether you are connecting to a secured Kafka listener or not. As a result, the values for `security.protocol` are:\n\n  * `PLAINTEXT` - using PLAINTEXT transport layer & no authentication - default value.\n  * `SSL` - using SSL transport layer & certificate-based authentication or no authentication.\n  * `SASL_PLAINTEXT` - using PLAINTEXT transport layer & SASL-based authentication.\n  * `SASL_SSL` - using SSL transport layer & SASL-based authentication.\n\nBased on the above, the security protocol you will use to connect to the different Kafka listeners that IBM Event Streams deploys are:\n\n   - `PLAINTEXT` when connecting to the non secured internal `plain` Kafka listener on port `9092`\n   - `SSL` when connecting to the secured (TLS encrypted) internal `tls` Kafka listener on port `9093` that also enforces authentication through TLS certificates\n   - `SASL_SSL` when connecting to the secured (TLS encrypted) external `external` Kafka listener on port `9094` that also enforces authentication through SCRAM credentials.\n\n### Non-secured listener\n\nYou would only need to specify that there is no security in place for your application to connect to a non-secured kafka listener:\n\n```properties\nsecurity.protocol=PLAINTEXT\n```\n\n### Secured listener\n\nIn order for your application to be able to connect to Kafka through the internal secured (TLS encrypted) Kafka listener, you need to set the appropriate value for `security.protocol` as seen above plus provide the Certificate Authority of the Kafka cluster (its public key).\n\nDepending on the technology of your application, you will need to provide the Certificate Authority of the Kafka cluster for the TLS encryption either as a `PKCS12` certificate for a Java client or as a `PEM` certificate for anything else. `PKCS12` certificates (or truststores) come in the form of a `.p12` file and are secured with a password. You can inspect a `PKCS12` certificate with:\n\n```sh\nopenssl pkcs12 -info -nodes -in truststore.p12\n```\n\nand providing the truststore password.\n\nAn example of the output would be:\n\n```sh\nMAC Iteration 100000\nMAC verified OK\nPKCS7 Encrypted data: Certificate bag\nBag Attributes\n    friendlyName: ca.crt\n    2.16.840.1.113894.746875.1.1: <Unsupported tag 6>\nsubject=/O=io.strimzi/CN=cluster-ca v0\nissuer=/O=io.strimzi/CN=cluster-ca v0\n-----BEGIN CERTIFICATE-----\nMIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL\nBQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2\nMDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv\nLnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB\nAQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx\nOF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j\ndM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c\nBj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk\nH2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD\n25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud\nDgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8\nRkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa\n/aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq\n3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag\njxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0\nJsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n```\n\nOn the other hand, `PEM` certificates come in the form of a `.pem` file and are not password protected.\n\nYou can inspect them using `cat`. \n\nThe output should be the same certificate as the one provided within the `PKCS12` certificate:\n\n```sh\ncat es-cert.pem \n-----BEGIN CERTIFICATE-----\nMIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL\nBQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2\nMDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv\nLnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB\nAQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx\nOF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j\ndM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c\nBj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk\nH2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD\n25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud\nDgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8\nRkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa\n/aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq\n3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag\njxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0\nJsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n```\n\nYou can find in the [Event Streams with CP4i](../event-streams/es-cp4i/#get-event-streams-tls-certificates) section how to obtain the Certificate Authority of your IBM Event Streams instance.\n\nOnce you have the Certificate Authority of your Kafka cluster, you will provide its location and password in your properties file through the `ssl.truststore.location` and `ssl.truststore.password` properties.\n\n\n```properties\nsecurity.protocol=SSL or SASL_SSL\nssl.protocol=TLSv1.2\n\nssl.truststore.password=<truststore.p12-password>\nssl.truststore.location=truststore.p12\nssl.truststore.type=PKCS12\n```\n\nwhere `security.protocol` will vary between `SSL` or `SASL_SSL` based on the authentication as you will see next.\n\n### Authentication\n\nYou have seen above that your Kafka listeners can require authentication to any application or client wanting to connect to the Kafka cluster through them. It was also said that authentication could be either of type SASL-based, through SCRAM (modern Salted Challenge Response Authentication Mechanism) credentials, or certificate-based (TLS). Either way, IBM Event Streams will handle authentication through `KafkaUser` objects. \n\nThese objects that represent Kafka users of your IBM Event Streams instance will have their authentication (and authorization through ACLs) credentials or TLS certificates associated to them stored in a secret. In order to find out how to create these `KafkaUsers`, which will vary depending on the authentication method, check out [this section](../event-streams/es-cp4i#generate-scram-service-credentials).\n\n#### Scram\n\nIf you have created a `KafkaUser` to be used with a Kafka listener that requires SCRAM authentication, you will be able to retrieve its SCRAM credentials either from the IBM Event Streams UI at creation time or later on from the secret these are stored to:\n\n```sh\noc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys=sasl.jaas.config --to=-\n```\n\nwhere\n\n- `<KAFKA_USER>` is the name of the `KafkaUser` object you created.\n- `<NAMESPACE>` is the namespace where IBM Event Streams is deployed on.\n\n  Example:\n\n  ```sh\n  oc extract secret/test-app -n tools --keys=sasl.jaas.config --to=-\n  # sasl.jaas.config\n  org.apache.kafka.common.security.scram.ScramLoginModule required username=\"test-app\" password=\"VgWpkjAkvxH0\";\n  ```\n\n  You can see above your SCRAM username and password.\n\n#### TLS\n\nIf you have created a `KafkaUser` to be used with a Kafka listener that requires TLS authentication, \nyou will be able to retrieve its TLS certificates either from the IBM Event Streams UI at creation time in a zip folder or \nlater on from the secret these are stored to.\n\nFirst, describe the secret to see what certificates are stored in it:\n\n```sh\n$ oc describe secret test-app-tls -n tools\nName:         test-app-tls\nNamespace:    tools\nLabels:       app.kubernetes.io/instance=test-app-tls\n              app.kubernetes.io/managed-by=strimzi-user-operator\n              app.kubernetes.io/name=strimzi-user-operator\n              app.kubernetes.io/part-of=eventstreams-test-app-tls\n              eventstreams.ibm.com/cluster=es-inst\n              eventstreams.ibm.com/kind=KafkaUser\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\nuser.key:       1704 bytes\nuser.p12:       2384 bytes\nuser.password:  12 bytes\nca.crt:         1180 bytes\nuser.crt:       1025 bytes\n```\n\nYou can see that the secret will store the following:\n\n- `user.key` and `user.crt` - the client certificate key-pair.\n- `user.p12` - trustore that contains the `user.key` and `user.crt`.\n- `user.password` - contains the `user.p12` truststore password.\n- `ca.crt` - CA used to sign the client certificate key-pair.\n\nThen, you can extract the appropriate certificate based on whether your application or Kafka client is Java based or not. In the case of a Java based application or Kafka client, extract the `user.p12` and `user.password`:\n\n```sh\noc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys=user.p12\noc extract secret/<KAFKA_USER> -n <NAMESPACE> --keys=user.password\n```\n\nwhere\n\n- `<KAFKA_USER>` is the name of the `KafkaUser` object you created.\n- `<NAMESPACE>` is the namespace where IBM Event Streams is deployed on.\n\n#### Properties config\n\nNow that you know how to get the authentication credentials or certificates for a proper authentication of your application or Kafka client you need to configure the appropriate properties for that:\n\n- If your Kafka listener authentication method is SCRAM:\n\n  ```properties\n  security.protocol=SASL_SSL\n\n  sasl.mechanism=SCRAM-SHA-512\n  sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<USERNAME>\" password\\=\"<PASSWORD>\";\n  ```\n\n- If your Kafka listener authentication method is TLS:\n\n  ```properties\n  security.protocol=SSL\n\n  ssl.keystore.location=<location_to_your_user.p12>\n  ssl.keystore.password=<user.p12-password>\n  ssl.keystore.type=PKCS12\n  ```\n\n## Recapitulation\n\nLet's have a full look at how the Kafka communication properties, for a Java application or client, would look like for IBM Event Streams \non RedHat OpenShift with the defaults. If you look at the IBM Event Streams instance deployment sample definitions \nin [this](https://github.com/ibm-messaging/event-streams-operator-resources/tree/master/cr-examples/eventstreams) GitHub repository, \nthat is mentioned in the IBM Event Streams official documentation [here](https://ibm.github.io/event-streams/installing/configuring/), \nyou will see that the defaults defined for the Kafka listeners for all of them (except from the `light-insecure.yaml` sample) are:\n\n```yaml\nlisteners:\n  external:\n    type: route\n    authentication:\n      type: scram-sha-512\n  tls:\n    authentication:\n      type: tls\n```\n\nThis translates to Strimzi (the open source project IBM Event Streams is based on) in:\n\n```yaml\nlisteners:\n  - name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n        type: tls\n  - name: external\n    type: route\n    port: 9094\n    tls: true \n    authentication:\n      type: scram-sha-512\n```\n\nLet's also add the `plain` non-secure Kafka listener to the picture so that all cases are covered in this recap section.\n\n```yaml\nlisteners:\n  plain: {}\n  external:\n    type: route\n    authentication:\n      type: scram-sha-512\n  tls:\n    authentication:\n      type: tls\n```\n\nAs a result, the IBM Event Streams instance deployed will count with:\n\n* One internal non secured kafka listener on port `9092` called `plain`\n* One internal secured (TLS encrypted) Kafka listener on port `9093` called `tls`, which also enforces authentication throughout TLS, and \n* One external secured (TLS encrypted) Kafka listener on port `9094` called `external`, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.\n\n### Plain\n\nThe Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the non secured kafka listener on port `9092` called `plain` will be as follows:\n\n```properties\n# Internal plain listener\n# =======================\nsecurity.protocol=PLAINTEXT\nbootstrap.servers=<ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9092\n```\n\nwhere\n\n- `<ES_NAME>` is the name of the IBM Event Streams instance deployed you are trying to connect to.\n- `<NAMESPACE>` is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.\n\n### Internal tls\n\nThe Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the internal s\necured (TLS encrypted) Kafka listener on port `9093` called `tls`, which also enforces authentication throughout mTLS will be as follows:\n\n```properties\n# Internal tls listener\n# =====================\nbootstrap.servers=<<ES_NAME>-kafka-bootstrap.<NAMESPACE>.svc\\:9093\n\nsecurity.protocol=SSL\nssl.protocol=TLSv1.2\n\n## mTLS Authentication for the client.\nssl.keystore.location=<user.p12-location>\nssl.keystore.password=<user.p12-password>\nssl.keystore.type=PKCS12\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=<trustore.p12-password>\nssl.truststore.location=<truststore.p12-location>\nssl.truststore.type=PKCS12\n```\n\nwhere\n\n- `<ES_NAME>` is the name of the IBM Event Streams instance deployed you are trying to connect to.\n- `<NAMESPACE>` is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.\n- `<user.p12-location>` is the location of the `user.p12` truststore containing the `user.key` and `user.crt` client certificate key-pair for the application or client mTLS authentication as explained above.\n- `<user.p12-password>` is the password of the `<user.p12>` truststore.\n- `<truststore.p12-location>` is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client.\n- `<trustore.p12-password>` is the password for the `truststore.p12` truststore.\n\nWhen the application is deployed on OpenShift, certificates will be mounted to the application pod. Below is an example of a Quarkus app deployment \ndescriptor, with environment variables:\n\n```yaml\nenv:\n  - name: KAFKA_SSL_TRUSTSTORE_FILE_LOCATION\n  value: /deployments/certs/server/ca.p12\n- name: KAFKA_SSL_TRUSTSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SSL_KEYSTORE_FILE_LOCATION\n  value: /deployments/certs/user/user.p12\n- name: KAFKA_SSL_KEYSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SECURITY_PROTOCOL\n  value: SSL\n- name: KAFKA_USER\n  value: tls-user\n- name: KAFKA_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: ca.password\n      name: kafka-cluster-ca-cert\n- name: USER_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: user.password\n      name: tls-user\n# ...\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: kafka-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n      volumes:\n      - name: kafka-cert\n        secret:\n          optional: true\n          secretName: kafka-cluster-ca-cert\n      - name: user-cert\n        secret:\n          optional: true\n          secretName: tls-user\n```\n\n### External tls\n\nThe Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the external secured (TLS encrypted) Kafka listener on port `9094` called `external`, which also enforces authentication throughout SCRAM credentials, and that is exposed through a route will be as follows:\n\n```properties\n# External listener SCRAM\n# =======================\nbootstrap.servers=<ES_NAME>-kafka-bootstrap-<NAMESPACE>.<OPENSHIFT_APPS_DNS>\\:443\n\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=<trustore.p12-password>\nssl.truststore.location=<truststore.p12-location>\nssl.truststore.type=PKCS12\n\n## Scram credentials\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"<SCRAM_USERNAME>\" password\\=\"<SCRAM_PASSWORD>\";\n```\n\nwhere\n\n- `<ES_NAME>` is the name of the IBM Event Streams instance deployed you are trying to connect to.\n- `<NAMESPACE>` is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.\n-  `<OPENSHIFT_APPS_DNS>` is your RedHat OpenShift DNS domain for application routes.\n- `<truststore.p12-location>` is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client.\n- `<trustore.p12-password>` is the password for the `truststore.p12` truststore.\n- `<SCRAM_USERNAME>` and `<SCRAM_PASSWORD>` are your SCRAM credentials.\n\n## Tips\n\nRemember that if the application does not run in the same namespace as the kafka cluster then you need to copy the secrets so that \nthe application developers can access the required credentials and certificates from their own namespaces with something like\n\n```sh\nif [[ -z $(oc get secret ${TLS_USER} 2> /dev/null) ]]\nthen\n   # As the project is personal to the user, we can keep a generic name for the secret\n   oc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2> /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n```\n\n## Kafka Connect\n\nFor Kafka connector, you need to define authentication used to connect to the Kafka Cluster:\n\n```yaml\n  authentication: \n    type: tls\n    certificateAndKey:\n      secretName: tls-user\n      certificate: user.crt\n      key: user.key\n```\n\n\n* Get TLS public cluster certificate:\n\n```yaml\n  tls: \n    trustedCertificates:\n      - secretName: dev-cluster-ca-cert\n        certificate: ca.crt\n```\n\n## Working with certificates\n\nTo extract a PEM-based certificate from a JKS-based truststore, you can use the following command:\n\n```sh\nkeytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n```\n\nTo build a PKCS12 from a pem do\n\n```sh\nopenssl pkcs12 -export -in cert.pem -out cert.p12\n# if you want jks\nkeytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks\n```","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/security/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}