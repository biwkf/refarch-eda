{"componentChunkName":"component---src-pages-technology-kafka-producers-index-mdx","path":"/technology/kafka-producers/","result":{"pageContext":{"frontmatter":{"title":"Kafka Producers","description":"Kafka Producers"},"relativePagePath":"/technology/kafka-producers/index.mdx","titleType":"append","MdxNode":{"id":"8d51fb30-718d-5c50-8b59-5a6424ee1132","children":[],"parent":"419efdcc-8f53-5d61-8ae1-81ca694ab8fa","internal":{"content":"---\ntitle: Kafka Producers\ndescription: Kafka Producers\n---\n\n<AnchorLinks>\n  <AnchorLink>Understanding Kafka Producers</AnchorLink>\n  <AnchorLink>Design considerations</AnchorLink>\n  <AnchorLink>Typical producer code structure</AnchorLink>\n  <AnchorLink>Advanced producer guidances</AnchorLink>\n</AnchorLinks>\n\n## Understanding Kafka Producers\n\nA producer is a thread safe kafka client API that publishes records to the \ncluster. It uses buffers, thread pool, and serializers to send data. \nThey are stateless: the consumers is responsible to manage the offsets of \nthe message they read. When the producer connects via the initial bootstrap connection, \nit gets the metadata about the topic - partition and the leader broker to connect to. \nThe assignment of messages to partition is done following different algorithms: \nround-robin if there is no key specified, using the hash code of the key, or custom defined.\n\nWe recommend reading [IBM Event streams producer guidelines](https://ibm.github.io/event-streams/about/producing-messages/) to understand \nhow producers work with its configuration parameters.\n\n## Design considerations\n\nWhen developing a record producer you need to assess the followings:\n\n* What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects?  Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions [from Martin Kleppmann on confluent web site](https://www.confluent.io/blog/put-several-event-types-kafka-topic/). Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section).\n* Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible [avro schema](https://avro.apache.org/docs/1.8.1/spec.html).\n* What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with `buffer.memory` property. (See [producer configuration API](https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/producer/ProducerConfig.html)\n* Can the producer batches events together to send them in batch over one send operation? By design kafka producers batch events.\n* Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection.\n* When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic.\n\n* Assess *exactly once* delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see [section](#how-to-support-exactly-once-delivery) below).\n* Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition.\n* Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that `LogAppendTime` is considered to be processing time, and `CreateTime` is considered to be event time.\n\n## Typical producer code structure\n\nThe producer code, using java or python API, does the following steps:\n\n* define producer properties\n* create a producer instance\n* Connect to the bootstrap URL, get a broker leader\n* send event records and get resulting metadata.\n\nProducers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements.\n\n[Here is an example of producer code from the quick start.](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/quarkus-kafka-producer)\n\n## Kafka useful Producer APIs\n\nHere is a list of common API to use in your producer and consumer code.\n\n* [KafkaProducer](https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html) A Kafka client that publishes records to the Kafka cluster.  The send method is asynchronous. A producer is thread safe so we can have per topic to interface.\n* [ProducerRecord](https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html) to be published to a topic\n* [RecordMetadata](https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html) metadata for a record that has been acknowledged by the server.\n\n### Properties to consider\n\nThe following properties are helpful to tune at each topic and producer and will vary depending on the requirements:  \n\n | Properties | Description |\n | --- | --- |\n | BOOTSTRAP_SERVERS_CONFIG |  A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers |\n | KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG |convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer.|\n | ACKS_CONFIG | specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. |\n | RETRIES_CONFIG | specifies the number of times to attempt to resend a batch of events. |\n | ENABLE_IDEMPOTENCE_CONFIG | Set to true, the number of retries will be maximized, and the acks will be set to `All`.|  \n | TRANSACTION_ID | A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session.|\n\n## Advanced producer guidances\n\n### How to support exactly once delivery\n\nKnowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events.\n\nProducer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported:\n\n* **At least once**: means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3).  If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message.\n* **At most semantic**: means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost.\n* **Exactly once** means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event.\n\nAt the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end.\n\n![](./images/exactly-one-0.png)\n\nSometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate...\n\n![](./images/exactly-one-1.png)\n\nTo avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost.\n\n![Exactly once](./images/exactly-one-2.png)\n\nThe sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system.\n\n> The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas.\n> As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance.\n\nTo add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transacitional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method:\n\n```java\nproducerProps.put(\"enable.idempotence\", \"true\");\nproducerProps.put(\"transactional.id\", \"prod-1\");\nkafkaProducer.initTransactions()\n```\n\n`initTransactions()` registers the producer with the broker as one that can use transaction, identifying it by its `transactional.id` and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work.\n\nKafka streams with consume-process-produce loop requires transaction and exactly once. Even commiting its read offset is part of the transaction. So Producer API has a [sendOffsetsToTransaction method](https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#sendOffsetsToTransaction-java.util.Map-org.apache.kafka.clients.consumer.ConsumerGroupMetadata-).\n\nSee the [KIP 98 for details.](https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging)\n\nIn case of multiple partitions, the broker will store a list of all updated partitions for a given transaction.\n\nTo support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. \n\nSee the [code in order command](https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/53bbb8cdeac413883ca2ccf521eb0797a43f45a3/order-command-ms/src/main/java/ibm/gse/orderms/infrastructure/kafka/OrderCommandProducer.java#L46) microservice.\n\nThe consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration\n\n```java\nconsumerProps.put(\"enable.auto.commit\", \"false\");\nconsumerProps.put(\"isolation.level\", \"read_committed\");\n```\n\nWith `read_committed`, no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written.\n\nIn consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read.\n\n```java\noffsetsToCommit.put(partition, new OffsetAndMetadata(offset + 1))\nproducer.sendOffsetsToTransaction(offsetsToCommit, \"consumer-group-id\");\n```\n\nThe producer then commits the transaction.\n\n```java\ntry {\n    kafkaProducer.beginTransaction();\n    ProducerRecord<String, String> record = new ProducerRecord<>(ApplicationConfig.ORDER_COMMAND_TOPIC, key, value);\n    Future<RecordMetadata> send = kafkaProducer.send(record, callBackFunction);\n\n    kafkaProducer.commitTransaction();\n} catch (KafkaException e){\n    kafkaProducer.abortTransaction();\n}\n```\n\nThere is an interesting [article](https://www.baeldung.com/kafka-exactly-once) from the Baeldung team about exactly once processing in kafka with code example which we have re-used to implement the order processing in our [Reefer Container Shipment reference application](https://ibm-cloud-architecture.github.io/refarch-kc/) and explained [here](https://ibm-cloud-architecture.github.io/refarch-kc/orders/order/)\n\n### Code Examples\n\n* [Order management with CQRS in Java](https://github.com/ibm-cloud-architecture/refarch-kc-order-ms)\n* [EDA quickstart Quarkus Producer API](https://github.com/ibm-cloud-architecture/eda-quickstart)\n* [Springboot with kafka template](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms)\n* [Event driven microservice template](https://github.com/jbcodeforce/microprofile-event-driven-microservice-template/)\n\n## More readings\n\n* [Creating advanced kafka producer in java - Cloudurable](http://cloudurable.com/blog/kafka-tutorial-kafka-producer-advanced-java-examples/index.html)\n* [Confluent blog: Exactly-once Semantics are Possible: Here’s How Kafka Does it](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/)\n\n","type":"Mdx","contentDigest":"3b4c345d6a67d0322da9c0ac54edee99","owner":"gatsby-plugin-mdx","counter":803},"frontmatter":{"title":"Kafka Producers","description":"Kafka Producers"},"exports":{},"rawBody":"---\ntitle: Kafka Producers\ndescription: Kafka Producers\n---\n\n<AnchorLinks>\n  <AnchorLink>Understanding Kafka Producers</AnchorLink>\n  <AnchorLink>Design considerations</AnchorLink>\n  <AnchorLink>Typical producer code structure</AnchorLink>\n  <AnchorLink>Advanced producer guidances</AnchorLink>\n</AnchorLinks>\n\n## Understanding Kafka Producers\n\nA producer is a thread safe kafka client API that publishes records to the \ncluster. It uses buffers, thread pool, and serializers to send data. \nThey are stateless: the consumers is responsible to manage the offsets of \nthe message they read. When the producer connects via the initial bootstrap connection, \nit gets the metadata about the topic - partition and the leader broker to connect to. \nThe assignment of messages to partition is done following different algorithms: \nround-robin if there is no key specified, using the hash code of the key, or custom defined.\n\nWe recommend reading [IBM Event streams producer guidelines](https://ibm.github.io/event-streams/about/producing-messages/) to understand \nhow producers work with its configuration parameters.\n\n## Design considerations\n\nWhen developing a record producer you need to assess the followings:\n\n* What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects?  Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions [from Martin Kleppmann on confluent web site](https://www.confluent.io/blog/put-several-event-types-kafka-topic/). Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section).\n* Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible [avro schema](https://avro.apache.org/docs/1.8.1/spec.html).\n* What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with `buffer.memory` property. (See [producer configuration API](https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/producer/ProducerConfig.html)\n* Can the producer batches events together to send them in batch over one send operation? By design kafka producers batch events.\n* Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection.\n* When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic.\n\n* Assess *exactly once* delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see [section](#how-to-support-exactly-once-delivery) below).\n* Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition.\n* Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that `LogAppendTime` is considered to be processing time, and `CreateTime` is considered to be event time.\n\n## Typical producer code structure\n\nThe producer code, using java or python API, does the following steps:\n\n* define producer properties\n* create a producer instance\n* Connect to the bootstrap URL, get a broker leader\n* send event records and get resulting metadata.\n\nProducers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements.\n\n[Here is an example of producer code from the quick start.](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/quarkus-kafka-producer)\n\n## Kafka useful Producer APIs\n\nHere is a list of common API to use in your producer and consumer code.\n\n* [KafkaProducer](https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html) A Kafka client that publishes records to the Kafka cluster.  The send method is asynchronous. A producer is thread safe so we can have per topic to interface.\n* [ProducerRecord](https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html) to be published to a topic\n* [RecordMetadata](https://kafka.apache.org/11/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html) metadata for a record that has been acknowledged by the server.\n\n### Properties to consider\n\nThe following properties are helpful to tune at each topic and producer and will vary depending on the requirements:  \n\n | Properties | Description |\n | --- | --- |\n | BOOTSTRAP_SERVERS_CONFIG |  A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers |\n | KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG |convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer.|\n | ACKS_CONFIG | specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. |\n | RETRIES_CONFIG | specifies the number of times to attempt to resend a batch of events. |\n | ENABLE_IDEMPOTENCE_CONFIG | Set to true, the number of retries will be maximized, and the acks will be set to `All`.|  \n | TRANSACTION_ID | A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session.|\n\n## Advanced producer guidances\n\n### How to support exactly once delivery\n\nKnowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events.\n\nProducer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported:\n\n* **At least once**: means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3).  If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message.\n* **At most semantic**: means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost.\n* **Exactly once** means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event.\n\nAt the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end.\n\n![](./images/exactly-one-0.png)\n\nSometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate...\n\n![](./images/exactly-one-1.png)\n\nTo avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost.\n\n![Exactly once](./images/exactly-one-2.png)\n\nThe sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system.\n\n> The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas.\n> As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance.\n\nTo add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transacitional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method:\n\n```java\nproducerProps.put(\"enable.idempotence\", \"true\");\nproducerProps.put(\"transactional.id\", \"prod-1\");\nkafkaProducer.initTransactions()\n```\n\n`initTransactions()` registers the producer with the broker as one that can use transaction, identifying it by its `transactional.id` and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work.\n\nKafka streams with consume-process-produce loop requires transaction and exactly once. Even commiting its read offset is part of the transaction. So Producer API has a [sendOffsetsToTransaction method](https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#sendOffsetsToTransaction-java.util.Map-org.apache.kafka.clients.consumer.ConsumerGroupMetadata-).\n\nSee the [KIP 98 for details.](https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging)\n\nIn case of multiple partitions, the broker will store a list of all updated partitions for a given transaction.\n\nTo support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. \n\nSee the [code in order command](https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/53bbb8cdeac413883ca2ccf521eb0797a43f45a3/order-command-ms/src/main/java/ibm/gse/orderms/infrastructure/kafka/OrderCommandProducer.java#L46) microservice.\n\nThe consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration\n\n```java\nconsumerProps.put(\"enable.auto.commit\", \"false\");\nconsumerProps.put(\"isolation.level\", \"read_committed\");\n```\n\nWith `read_committed`, no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written.\n\nIn consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read.\n\n```java\noffsetsToCommit.put(partition, new OffsetAndMetadata(offset + 1))\nproducer.sendOffsetsToTransaction(offsetsToCommit, \"consumer-group-id\");\n```\n\nThe producer then commits the transaction.\n\n```java\ntry {\n    kafkaProducer.beginTransaction();\n    ProducerRecord<String, String> record = new ProducerRecord<>(ApplicationConfig.ORDER_COMMAND_TOPIC, key, value);\n    Future<RecordMetadata> send = kafkaProducer.send(record, callBackFunction);\n\n    kafkaProducer.commitTransaction();\n} catch (KafkaException e){\n    kafkaProducer.abortTransaction();\n}\n```\n\nThere is an interesting [article](https://www.baeldung.com/kafka-exactly-once) from the Baeldung team about exactly once processing in kafka with code example which we have re-used to implement the order processing in our [Reefer Container Shipment reference application](https://ibm-cloud-architecture.github.io/refarch-kc/) and explained [here](https://ibm-cloud-architecture.github.io/refarch-kc/orders/order/)\n\n### Code Examples\n\n* [Order management with CQRS in Java](https://github.com/ibm-cloud-architecture/refarch-kc-order-ms)\n* [EDA quickstart Quarkus Producer API](https://github.com/ibm-cloud-architecture/eda-quickstart)\n* [Springboot with kafka template](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms)\n* [Event driven microservice template](https://github.com/jbcodeforce/microprofile-event-driven-microservice-template/)\n\n## More readings\n\n* [Creating advanced kafka producer in java - Cloudurable](http://cloudurable.com/blog/kafka-tutorial-kafka-producer-advanced-java-examples/index.html)\n* [Confluent blog: Exactly-once Semantics are Possible: Here’s How Kafka Does it](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/)\n\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/kafka-producers/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}