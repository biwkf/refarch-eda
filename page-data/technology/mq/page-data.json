{"componentChunkName":"component---src-pages-technology-mq-index-mdx","path":"/technology/mq/","result":{"pageContext":{"frontmatter":{"title":"IBM MQ in the context of EDA"},"relativePagePath":"/technology/mq/index.mdx","titleType":"append","MdxNode":{"id":"b990325b-4188-54e7-b95f-fa9a04a1b3a8","children":[],"parent":"8f821612-ab46-5609-afd7-e630a331f1b7","internal":{"content":"---\ntitle: IBM MQ in the context of EDA\n---\n\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 3//4/2021- Work in progress</strong>\n</InlineNotification>\n\n\n[IBM MQ](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.2.0/com.ibm.mq.pro.doc/q001020_.htm) is the enterprise solution to exchange message over queues. \nAs it supports loosely coupling communication between applications, via asynchronous protocol, it is part of the event driven architecture product portfolio and makes senses to continue to include it as part of modern cloud native solutions. \n\nThis note is to summarize, for architect, the technology as it fits into EDA and gives pointers to important documentations, articles, and code repositories for using MQ.\n\nWe already addressed the [difference between event and messaging](../../concepts/events-versus-messages) systems, and we can affirm that real production plaform needs to include both. This site includes a lot on Kafka as the backbone to support EDA, but MQ delivers a very important missing element for Kafka, is the transactional support to write message to queue for ensuring message delivery. \n\nMQ queue managers are the main component to define queue and where applications connect to. They can be organized in network to deliver messages between applications and locations. Managers can be organized in cluster to increase high availability and scaling.\n\n ![1](./images/Picture1.png)\n\nWe encourage to read the [article from Richard Coppen's: 'IBM MQ fundamentals'](https://developer.ibm.com/components/ibm-mq/articles/mq-fundamentals).\n\n## Major benefits in EDA\n\n* **MQ provides assured delivery of data**: No data loss and no duplication, strong support of exactly once.\n* **MQ is horizontally scalable**: As the workload for a single queue manager increases, it is easy to add more queue managers to share tasks and distribute the messages across them. \n* **Highly available**\n* Integrate well with Mainframe to propagate transaction to eventual consistenty world of cloud native distributed applications. Writing to database and MQ queue is part of the same transaction, which simplifies the injection into event backbone like Kafka, via MQ connector.\n* Containerized to run on modern kubernetes platform.\n\n### Decentralized architecture\n\nThe figure below illustrates the different ways to organize the MQ brokers according to the applications' needs.\n\n![](./images/decentralized.png)\n\n* On the top row applications have decoupled queue managers, with independent availability / scalability. The ownership is decentralized, as each application owner also own the broker configuration and deployment. Such cloud native application may adopt the [Command Query Responsability Seggregation](/patterns/cqrs/) pattern and use queues to propagage information between the microservices. The deploy of both broker and microservices follow the same CI/CD pipeline, with a kustomize to describe the broker configuration. See the [CQRS with MQ implementation](https://github.com/ibm-cloud-architecture/vaccine-reefer-mgr-cmd) we did for the Reefer manager service in the vaccine solution.  \n* A central MQ broker can still be part of the architecture to support legacy application and federated queues. \n\nThis type of deployment supports heterogenous operational procedures across technologies. \n\n### Scaling and High availability\n\nA set of topology can be defined to support HA:\n\n![topology](./images/mq-topologies.png)\n\n1. Single resilient queue manager: MQ broker runs in a VM or a single container, and if it stops the VM or pod scheduler will restart it. This is using the platform resynch capability combined with HA storage. IP Address is kept between the instance. The queue content is saved to a storage supporting HA. In the case of container, new restarted pod will connect to existing storage, and the IP gateway routes traffic to the active instance via service and app selector.\n1. Multi-instance queue manager: active - standby topology - Failover is triggered on failure of the active instance. IP Address is also kept. When using k8s, the stand-by broker is on a separate node, ready to be activated.\n1. replicated data queue manager: this is an extension of the previous pattern where data saved locally is replicated to other sites. Not supported in the k8s.\n\nThe deployed MQ broker is defined in k8s as a `StatefulSet` which may not restart automatically in case of node failure. So there is a time to fail over, which is not the case with the full replication mechanism of Kafka.\n\n### Disaster recovery\n\n## Installation with Cloud Pak for Integration\n\nStarting with release 2020.2, MQ can be installed via Kubernetes Operator on Openshift platform. From the operator catalog search for MQ. See the [product documentation installation guide](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.2.0/com.ibm.mq.ctr.doc/ctr_installing_ui.htm) for up to date details.\n\nYou can verify your installation with the following CLI, and get the IBM catalogs accessible:\n\n```shell\noc project openshift-marketplace\noc get CatalogSource\nNAME                   DISPLAY                TYPE      PUBLISHER     AGE\ncertified-operators    Certified Operators    grpc      Red Hat       42d\ncommunity-operators    Community Operators    grpc      Red Hat       42d\nibm-operator-catalog   ibm-operator-catalog   grpc      IBM Content   39d\nopencloud-operators    IBMCS Operators        grpc      IBM           39d\nredhat-marketplace     Red Hat Marketplace    grpc      Red Hat       42d\nredhat-operators       Red Hat Operators      grpc      Red Hat       42d\n```\n\nOnce everything is set up, create an operator. The IBM MQ operator can be installed scoped to a single namespace or all namespaces.  \n \n <img src=\"./images/mq-operator.png\" width=\"200px\"></img>\n\nVerify your environment fits the deployment. [Prepare your Red Hat OpenShift Container Platform for MQ](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.2.0/com.ibm.mq.ctr.doc/ctr_prepare_project_ui.htm)\nThen once the operator is installed (it could take up to a minute), go to the operator page and create a MQ Manager instance. For example be sure to have defined an ibm-entitlement-key in the project you are planning to use to deploy MQ manager\n\n <img src=\"./images/mq-mgr-via-operator.png\" width=\"200px\"></img>\n\nThen update the Yaml file for name, license and persistence.\n\n <img src=\"./images/mq-yaml.png\" width=\"200px\"></img>\n\nAs an alternate, define a QueueManager manifest yaml file as:\n\n ```yaml\napiVersion: mq.ibm.com/v1beta1\nkind: QueueManager\nmetadata:\n  name: eda-mq-lab\nspec:\n  version: 9.2.0.0-r1\n  license:\n    accept: true\n    license: L-RJON-BN7PN3\n    use: NonProduction\n  web:\n    enabled: true\n  queueManager:\n    name: \"EDAQMGR1\"\n    storage:\n      queueManager:\n        type: ephemeral\n  template:\n    pod:\n      containers:\n       - name: qmgr\n         env:\n         - name: MQSNOAUT\n           value: \"yes\"\n ```\n\n Then create the QueueManager resource: \n\n ```shell\n oc apply -f mq-manager.yaml \n # Get the UI route \n oc describe queuemanager eda-mq-lab\n ```\n\n You should get the console from this URL: https://eda-mq-lab-ibm-mq-web-....containers.appdomain.cloud/ibmmq/console/#/\n \n ![4](./images/mq-console.png)\n\nTo access to the `mqsc` CLI and run configuration remote connect via `oc exec -it <podname> bash`.\n\n## Running MQ in docker\n\nThe [following recent article](https://developer.ibm.com/tutorials/mq-connect-app-queue-manager-containers/) from Richard J. Coppen presents such deployment, and can be summarized as:\n\n```shell\n# Use Docker to create a volume:\ndocker volume create qm1data\n# Start queue manager: QM1\ndocker run --env LICENSE=accept --env MQ_QMGR_NAME=QM1 --volume qm1data:/mnt/mqm --publish 1414:1414 --publish 9443:9443 --detach --env MQ_APP_PASSWORD=passw0rd ibmcom/mq:latest\n# The queue manager’s listener listens on port 1414 for incoming connections and port 9443 is used by MQ console\n```\n\nOne queue is created DEV.QUEUE.1 and a channel: DEV.APP.SRVCONN. \n\nThen `docker exec` on the docker container and use the `mqsc` CLI.\n\nThe [ibm-messaging/mq-container](https://github.com/ibm-messaging/mq-container) github repository describes properties and different configurations.\n\n## Getting access to the MQ manager\n\nThe MQ Console is a web browser based interface for interacting with MQ objects. It comes pre-configured inside the developer version of MQ in a container. On localhost deployment the URL is [ https://localhost:9443/ibmmq/console/]( https://localhost:9443/ibmmq/console/) while on OpenShift it depends of the Route created.\n\n\n\n## Important readings\n\n* [MQ family page](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ/com.ibm.mq.helphome.doc/product_welcome_wmq.htm)\n* [Cloud Pack for Integration product documentation](https://www.ibm.com/support/knowledgecenter/en/SSGT7J_20.2/overview.html)\n* [MQ 9.2 product documentation](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.2.0/com.ibm.mq.helphome.v92.doc/WelcomePagev9r1.htm)\n* [Article for developer from Richard Coppen's: 'IBM MQ fundamentals'](https://developer.ibm.com/components/ibm-mq/articles/mq-fundamentals)\n* [MQ on Container](https://developer.ibm.com/tutorials/mq-connect-app-queue-manager-containers/)\n\n## Code repositories\n\n* []()","type":"Mdx","contentDigest":"9d80e1a8c280e1bd1fc1a4a2a43988e8","counter":720,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"IBM MQ in the context of EDA"},"exports":{},"rawBody":"---\ntitle: IBM MQ in the context of EDA\n---\n\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 3//4/2021- Work in progress</strong>\n</InlineNotification>\n\n\n[IBM MQ](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.2.0/com.ibm.mq.pro.doc/q001020_.htm) is the enterprise solution to exchange message over queues. \nAs it supports loosely coupling communication between applications, via asynchronous protocol, it is part of the event driven architecture product portfolio and makes senses to continue to include it as part of modern cloud native solutions. \n\nThis note is to summarize, for architect, the technology as it fits into EDA and gives pointers to important documentations, articles, and code repositories for using MQ.\n\nWe already addressed the [difference between event and messaging](../../concepts/events-versus-messages) systems, and we can affirm that real production plaform needs to include both. This site includes a lot on Kafka as the backbone to support EDA, but MQ delivers a very important missing element for Kafka, is the transactional support to write message to queue for ensuring message delivery. \n\nMQ queue managers are the main component to define queue and where applications connect to. They can be organized in network to deliver messages between applications and locations. Managers can be organized in cluster to increase high availability and scaling.\n\n ![1](./images/Picture1.png)\n\nWe encourage to read the [article from Richard Coppen's: 'IBM MQ fundamentals'](https://developer.ibm.com/components/ibm-mq/articles/mq-fundamentals).\n\n## Major benefits in EDA\n\n* **MQ provides assured delivery of data**: No data loss and no duplication, strong support of exactly once.\n* **MQ is horizontally scalable**: As the workload for a single queue manager increases, it is easy to add more queue managers to share tasks and distribute the messages across them. \n* **Highly available**\n* Integrate well with Mainframe to propagate transaction to eventual consistenty world of cloud native distributed applications. Writing to database and MQ queue is part of the same transaction, which simplifies the injection into event backbone like Kafka, via MQ connector.\n* Containerized to run on modern kubernetes platform.\n\n### Decentralized architecture\n\nThe figure below illustrates the different ways to organize the MQ brokers according to the applications' needs.\n\n![](./images/decentralized.png)\n\n* On the top row applications have decoupled queue managers, with independent availability / scalability. The ownership is decentralized, as each application owner also own the broker configuration and deployment. Such cloud native application may adopt the [Command Query Responsability Seggregation](/patterns/cqrs/) pattern and use queues to propagage information between the microservices. The deploy of both broker and microservices follow the same CI/CD pipeline, with a kustomize to describe the broker configuration. See the [CQRS with MQ implementation](https://github.com/ibm-cloud-architecture/vaccine-reefer-mgr-cmd) we did for the Reefer manager service in the vaccine solution.  \n* A central MQ broker can still be part of the architecture to support legacy application and federated queues. \n\nThis type of deployment supports heterogenous operational procedures across technologies. \n\n### Scaling and High availability\n\nA set of topology can be defined to support HA:\n\n![topology](./images/mq-topologies.png)\n\n1. Single resilient queue manager: MQ broker runs in a VM or a single container, and if it stops the VM or pod scheduler will restart it. This is using the platform resynch capability combined with HA storage. IP Address is kept between the instance. The queue content is saved to a storage supporting HA. In the case of container, new restarted pod will connect to existing storage, and the IP gateway routes traffic to the active instance via service and app selector.\n1. Multi-instance queue manager: active - standby topology - Failover is triggered on failure of the active instance. IP Address is also kept. When using k8s, the stand-by broker is on a separate node, ready to be activated.\n1. replicated data queue manager: this is an extension of the previous pattern where data saved locally is replicated to other sites. Not supported in the k8s.\n\nThe deployed MQ broker is defined in k8s as a `StatefulSet` which may not restart automatically in case of node failure. So there is a time to fail over, which is not the case with the full replication mechanism of Kafka.\n\n### Disaster recovery\n\n## Installation with Cloud Pak for Integration\n\nStarting with release 2020.2, MQ can be installed via Kubernetes Operator on Openshift platform. From the operator catalog search for MQ. See the [product documentation installation guide](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.2.0/com.ibm.mq.ctr.doc/ctr_installing_ui.htm) for up to date details.\n\nYou can verify your installation with the following CLI, and get the IBM catalogs accessible:\n\n```shell\noc project openshift-marketplace\noc get CatalogSource\nNAME                   DISPLAY                TYPE      PUBLISHER     AGE\ncertified-operators    Certified Operators    grpc      Red Hat       42d\ncommunity-operators    Community Operators    grpc      Red Hat       42d\nibm-operator-catalog   ibm-operator-catalog   grpc      IBM Content   39d\nopencloud-operators    IBMCS Operators        grpc      IBM           39d\nredhat-marketplace     Red Hat Marketplace    grpc      Red Hat       42d\nredhat-operators       Red Hat Operators      grpc      Red Hat       42d\n```\n\nOnce everything is set up, create an operator. The IBM MQ operator can be installed scoped to a single namespace or all namespaces.  \n \n <img src=\"./images/mq-operator.png\" width=\"200px\"></img>\n\nVerify your environment fits the deployment. [Prepare your Red Hat OpenShift Container Platform for MQ](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.2.0/com.ibm.mq.ctr.doc/ctr_prepare_project_ui.htm)\nThen once the operator is installed (it could take up to a minute), go to the operator page and create a MQ Manager instance. For example be sure to have defined an ibm-entitlement-key in the project you are planning to use to deploy MQ manager\n\n <img src=\"./images/mq-mgr-via-operator.png\" width=\"200px\"></img>\n\nThen update the Yaml file for name, license and persistence.\n\n <img src=\"./images/mq-yaml.png\" width=\"200px\"></img>\n\nAs an alternate, define a QueueManager manifest yaml file as:\n\n ```yaml\napiVersion: mq.ibm.com/v1beta1\nkind: QueueManager\nmetadata:\n  name: eda-mq-lab\nspec:\n  version: 9.2.0.0-r1\n  license:\n    accept: true\n    license: L-RJON-BN7PN3\n    use: NonProduction\n  web:\n    enabled: true\n  queueManager:\n    name: \"EDAQMGR1\"\n    storage:\n      queueManager:\n        type: ephemeral\n  template:\n    pod:\n      containers:\n       - name: qmgr\n         env:\n         - name: MQSNOAUT\n           value: \"yes\"\n ```\n\n Then create the QueueManager resource: \n\n ```shell\n oc apply -f mq-manager.yaml \n # Get the UI route \n oc describe queuemanager eda-mq-lab\n ```\n\n You should get the console from this URL: https://eda-mq-lab-ibm-mq-web-....containers.appdomain.cloud/ibmmq/console/#/\n \n ![4](./images/mq-console.png)\n\nTo access to the `mqsc` CLI and run configuration remote connect via `oc exec -it <podname> bash`.\n\n## Running MQ in docker\n\nThe [following recent article](https://developer.ibm.com/tutorials/mq-connect-app-queue-manager-containers/) from Richard J. Coppen presents such deployment, and can be summarized as:\n\n```shell\n# Use Docker to create a volume:\ndocker volume create qm1data\n# Start queue manager: QM1\ndocker run --env LICENSE=accept --env MQ_QMGR_NAME=QM1 --volume qm1data:/mnt/mqm --publish 1414:1414 --publish 9443:9443 --detach --env MQ_APP_PASSWORD=passw0rd ibmcom/mq:latest\n# The queue manager’s listener listens on port 1414 for incoming connections and port 9443 is used by MQ console\n```\n\nOne queue is created DEV.QUEUE.1 and a channel: DEV.APP.SRVCONN. \n\nThen `docker exec` on the docker container and use the `mqsc` CLI.\n\nThe [ibm-messaging/mq-container](https://github.com/ibm-messaging/mq-container) github repository describes properties and different configurations.\n\n## Getting access to the MQ manager\n\nThe MQ Console is a web browser based interface for interacting with MQ objects. It comes pre-configured inside the developer version of MQ in a container. On localhost deployment the URL is [ https://localhost:9443/ibmmq/console/]( https://localhost:9443/ibmmq/console/) while on OpenShift it depends of the Route created.\n\n\n\n## Important readings\n\n* [MQ family page](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ/com.ibm.mq.helphome.doc/product_welcome_wmq.htm)\n* [Cloud Pack for Integration product documentation](https://www.ibm.com/support/knowledgecenter/en/SSGT7J_20.2/overview.html)\n* [MQ 9.2 product documentation](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.2.0/com.ibm.mq.helphome.v92.doc/WelcomePagev9r1.htm)\n* [Article for developer from Richard Coppen's: 'IBM MQ fundamentals'](https://developer.ibm.com/components/ibm-mq/articles/mq-fundamentals)\n* [MQ on Container](https://developer.ibm.com/tutorials/mq-connect-app-queue-manager-containers/)\n\n## Code repositories\n\n* []()","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/mq/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}