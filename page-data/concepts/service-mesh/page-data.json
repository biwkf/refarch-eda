{"componentChunkName":"component---src-pages-concepts-service-mesh-index-mdx","path":"/concepts/service-mesh/","result":{"pageContext":{"frontmatter":{"title":"Microservice mesh"},"relativePagePath":"/concepts/service-mesh/index.mdx","titleType":"append","MdxNode":{"id":"5e58b1cc-e0c4-53ef-bb4c-6f9a68c9a930","children":[],"parent":"d291c432-0169-50e0-8ee9-bbb1355cfeda","internal":{"content":"---\ntitle: Microservice mesh\n---\n\nIn this note we are grouping the studies around microservice to microservice communication with Kubernetes deployment. We are addressing:\n\n* how ingress controller helps inside Kubernetes\n* how API gateway helps for API management and service integration\n* how to expose service in hybrid cloud\n* how to discover service\n\nAll come back to the requirements, skill set and fit to purpose.\n\n## Definitions\n\nService meshes provide visibility, resiliency, traffic, and security control of distributed application services. They deliver policy-based networking for microservices in the contraints of virtual network and continuous topology updates. Externalizing, via declarations, the logic to support network potential issues, like resiliency, simplifies dramatically developers work.   \n\nSome misconception to clarify around microservice and APIs:\n\n* microservices are not fine grained web services\n* APIs are not equivalent to microservices\n* microservices are not implementation of APIs\n\nAPI is an interface, a way to make a request to get or change data in an application. In modern use API refers to REST web APIs using HTTP protocol, with JSON format (sometime XML is still used). Interface decouples the caller from the implementation. The caller has no idea how it is implemented.\n\nA microservice is in fact a component. micro refers to the granularity of the component not of the exposed interface. The following diagram illustrates all those concepts.\n\n![](./images/ms-apis.png)\n\nWe encourage you to go to read [integration design and architecture series](https://developer.ibm.com/integration/blog).\n\nContainer orchestration like Kubernetes are mainly doing application scheduling, cluster management, resource provisioning, platform and workload monitoring and service discovery.\n\nWhen application solutions are growing in size and complexity, you need to addres the following items:\n\n* visibility on how traffic is flowing between microservice, how routing is done between microservice based on requests contained or the origination point or the end point\n* how to support resiliency by handling failure in a graceful manner\n* how to ensure security with identity assertion\n* how to enforce security policy   \n\nwhich defined the requirements for service mesh.\n\nService mesh architecture defines a data and control planes:\n\n* Control plane: supports policy and configuration for services in the mesh, and provides aggregation for telemetry. It has API and CLI to centralize control to the services deployed. In Kubernetes control planes are deployed in a system namespace.\n* Data plane: handles the actual inspection, transiting, and routing of network traffic. It is responsible for health checking, load balancing, authentication, authorization, inbound (ingress) and outbound (egress) cluster network traffic.\n\nApplications / microservices are unaware of data plane.\n\n## Context\n\nTraditional modern architecture involves having different components exposing reusable APIs, addressing different channels (mobile, single page application, traditional server pages or B2B apps), consuming APIs (mobile APIs, back end for front end, shared common apis like authentication, authorization,...) and backend services addressing reusable business services:\n\n![](./images/current-arch-compo.png)\n\nAPI management can be added via API gateway. This is a distributed application with cross related communication channels, where any changes to the service interface characteristics impact any of the components.\n\nMoving to microservices architecture style adds more communication challenges and devops complexity but provides a lot of business values such as:\n* rapid deployment of new business capabilities, co-evolving in parallel of other services.\n* focusing on business domain with clear ownership of the business function and feature roadmap\n* better operation procedure, automated, and with easy rollout and continuous delivery.\n* A/B testing to assess how new feature deployed improve business operations\n* Improve resiliency by deploying on multi language cluster\n\nAs an example we can use the [following predictive maintenance asset solution](https://github.com/ibm-cloud-architecture/refarch-asset-analytics) with the following capabilities to support:\n\n* user authentication\n* user management: add / delete new user\n* user self registration, reset password\n* user permission control\n* user profile\n* asset management\n* risk assessment service\n\nEach could be grouped by business domain like the user management, asset management, and application access control. So domain separation can be a good microservice boundary. But if the number of user reach millions then we may need to optimize the runtime processing of reading user credential, and scale the service differently, leading to a service map like the diagram below, where runtime and management are separated services.\n\n![](./images/ms-view.png)  \n\nAll of these still does not address the fact that data are distributed and even more with microservices owning their data persistence. As developers and architects we still have to address the following data integrity problems:\n\n* two phases commit\n* compensating operation\n* eventual data consistency: some microservice updating data may share those updates with other microservices.\n* Data aggregation: adding new views on data, owned by a microservice, to support new aggregates. Examples are preparing data view for machine learning modeling, analytics, or business intelligence...\n\nFrom the previous microservice allocation we can see the needs to propagate data update between services. Adding or unsubscribing a user involves updating the asset the user own and the authentication runtime service:\n\n![](./images/data-consistency.png)  \n\nAdding a new application changes the authorization runtime service.\n\nWe are now looking at the following questions:\n\n- how does webapp access APIs for their main service, of back end for front end service.\n- how does deployed microservice access other service: discover and access?\n- How data consistency can be ensured?\n- is there a simpler way to manage cross microservice dependency?\n\nThe answers depend on the existing infrastructure and environment, and deployment needs.\n\n## Service routing\n\nWe have to dissociate intra-cluster communication versus inter clusters or cluster to external services. Without getting into too much detail of IP routing within Kubernetes some important elements of the cluster are important to remember:\n* microservices are packaged as docker container and expose port. When deployed they run in a pod within a node (physical or virtual machine)\n* containers can talk to other containers only if they are on the same machine, or when they have exposed port.\n* Kubernetes is configured with a large flat subnet (e.g. 172.30.0.0/16) which is used for internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node.\nWhen a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node.\n\n![](./images/kube-pod-network.png)   \n\n* Kube-proxy intercepts and controls where to forward the traffic, either to another worker node running your destination pod, or outside of the cluster\n* Kube proxy watches the API Server on the Master Node for the addition and removal of Services endpoints. It configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints.\n* Worker nodes have internal DNS service and load balancer\n\nWithin Kubernetes, Ingress is a service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps. You use ingress when you need to support HTTP, HTTPS, TLS, load balancing, expose app outside of the cluster, and custom routing rules...\n\nOne ingress resource is required by namespace. So if microservices are in the same namespace you can define a domain name for those services (e.g. assetmanagement.greencompute.ibmcase.com) and defined path for each service:\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: assetmanagement\nspec:\n  rules:\n    - host: assetmanagement.greencompute.ibmcase.com\n      http:\n        paths:\n          - path: /assetconsumer\n            backend:\n              serviceName: asset-consumer-svc\n              servicePort: 8080\n          - path: /assetdashboard\n            backend:\n              serviceName: asset-dashboard-bff-svc\n              servicePort: 8080\n          - path: /assetmgrms\n            backend:\n              serviceName: asset-mgr-ms-svc\n              servicePort: 8080\n```\nThe backend for front end component, the asset manager microservice and the asset consumer components are exposed in the same domain.\nThe `serviceName` matches the service exposed for each components.\nThe following diagram presents how an external application accesses deployed microservice within Kubernetes pod.\n\nThe following diagram shows how Ingress directs communication from the internet to a deployed microservice:\n\n![](./images/ingress-routing.png)\n\n1. A user sends a request to your app by accessing your app's URL. Using DNS name abstracts the application from the underlying infrastructure. Inter clusters microservice to microservice should use the same approach\n1. A DNS system service resolves the hostname in the URL to the portable public IP address of the load balancer\n1. Based on the resolved IP address, the client sends the request to the load balancer service that exposes the Application Load Balancer (ALB)\n1. The ALB checks if a routing rule for the app path in the cluster exists. If a matching rule is found, the request is forwarded according to the rules that you defined in the Ingress resource to the pod where the app is deployed. If multiple app instances are deployed in the cluster, the ALB load balances the requests between the app pods. To also load balance incoming HTTPS connections, you can configure the ALB to you can use your own TLS certificate to decrypt the network traffic.\n1. Microservice to microservice can use this DNS name to communicate between service.\n\nUsing Ingress, the global load balancer can support parallel, cross region, clusters.\n\n## Service exposition\n\nThere is an architecture style focusing on APIs which proposes to have different SLA and semantic for external, internet facing API versus internal back end APIs only exposed within intranet. [This article](../hybrid-ref-arch.md)  presents using different API gateways to support this architecture.\n\nBackend data services are not exposed directly to internet. API Gateway provides a secure end point for external web app to access those business functions.\n\nSo the decisions on how to expose service are linked to:\n\n* do you need to do API management\n* do you need to secure APIs\n* do you need to expose to internet\n* do you need to support other protocol then HTTP\n* do you need to have multiple instance of the application\n\nWhen deploying a microservice to Kubernetes it is recommended to use Ingress rule as presented above.. The following yaml file exposes the BFF service using ClusterIP:\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: asset-consumer-svc\n  labels:\n    chart: asset-consumer\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: asset-consumer-svc\n  selector:\n    app: asset-consumer\n```\n\n## Service discovery\n\nWhen deploying on Kubernetes cluster, microservices use the DNS lookup to discover deployed microservice.\n\n\n## ISTIO\n\n[ISTIO](https://istio.io/) provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, traffic flow management, monitoring, etc...\nBy deploying a special sidecar proxy (called Envoy) throughout your environment, all network communication between microservices is intercepted and controlled by ISTIO control panel.\n\nThe control plane manages the overall network infrastructure and enforces the policy and traffic rules.\n\n### More reading\n\n[Istio and Kubernetes Workshop](https://github.com/IBM/istio101/tree/master/workshop)\n[Advanced traffic management with ISTIO](https://developer.ibm.com/code/patterns/manage-microservices-traffic-using-istio/)\n[Istio workshop for IBM Cloud Container service](https://github.com/szihai/istio-workshop)\n[ Our Istio FAQ](../istio/readme#faq)\n\n## Asynchronous loosely coupled solution using events\n\nIf we change of paradigm and use a messaging approach or better an event approach of the data update requirements, we will implement a loosly coupled solution with a pub/sub communication protocol. We need to think about the activities that apply within each service and how they can be of interest to other components. Internal microservice tasks are becoming facts about something happened and those facts may be published as events for others to consume. The first level of refactoring may become:  \n\n![](./images/pub-events.png)\n\nAn event is a fact that happens in the past and carry all the data needed, and it becomes a source of record. It becoming consistent if it is played in a messaging backbone via topics.\n\n![](./images/message-topics.png)  \n\nBut the persistence of data can be externalized to consumer and then simplify the architecture:\n\n![](./images/message-topics2.png)\n\n\nThen we can use the history of the persisted events to add features not address before, and outside of the direct scope of a microservice. For example to compute the number of users added last month, just a query on the users topic will get the answer: no or very limited coding needed.\n\nWe recommend to go deeper in event driven architecture [with this site](https://ibm-cloud-architecture.github.io/refarch-eda/)\n","type":"Mdx","contentDigest":"cf8d625c6141a74053f8c8e21ac54d7a","owner":"gatsby-plugin-mdx","counter":860},"frontmatter":{"title":"Microservice mesh"},"exports":{},"rawBody":"---\ntitle: Microservice mesh\n---\n\nIn this note we are grouping the studies around microservice to microservice communication with Kubernetes deployment. We are addressing:\n\n* how ingress controller helps inside Kubernetes\n* how API gateway helps for API management and service integration\n* how to expose service in hybrid cloud\n* how to discover service\n\nAll come back to the requirements, skill set and fit to purpose.\n\n## Definitions\n\nService meshes provide visibility, resiliency, traffic, and security control of distributed application services. They deliver policy-based networking for microservices in the contraints of virtual network and continuous topology updates. Externalizing, via declarations, the logic to support network potential issues, like resiliency, simplifies dramatically developers work.   \n\nSome misconception to clarify around microservice and APIs:\n\n* microservices are not fine grained web services\n* APIs are not equivalent to microservices\n* microservices are not implementation of APIs\n\nAPI is an interface, a way to make a request to get or change data in an application. In modern use API refers to REST web APIs using HTTP protocol, with JSON format (sometime XML is still used). Interface decouples the caller from the implementation. The caller has no idea how it is implemented.\n\nA microservice is in fact a component. micro refers to the granularity of the component not of the exposed interface. The following diagram illustrates all those concepts.\n\n![](./images/ms-apis.png)\n\nWe encourage you to go to read [integration design and architecture series](https://developer.ibm.com/integration/blog).\n\nContainer orchestration like Kubernetes are mainly doing application scheduling, cluster management, resource provisioning, platform and workload monitoring and service discovery.\n\nWhen application solutions are growing in size and complexity, you need to addres the following items:\n\n* visibility on how traffic is flowing between microservice, how routing is done between microservice based on requests contained or the origination point or the end point\n* how to support resiliency by handling failure in a graceful manner\n* how to ensure security with identity assertion\n* how to enforce security policy   \n\nwhich defined the requirements for service mesh.\n\nService mesh architecture defines a data and control planes:\n\n* Control plane: supports policy and configuration for services in the mesh, and provides aggregation for telemetry. It has API and CLI to centralize control to the services deployed. In Kubernetes control planes are deployed in a system namespace.\n* Data plane: handles the actual inspection, transiting, and routing of network traffic. It is responsible for health checking, load balancing, authentication, authorization, inbound (ingress) and outbound (egress) cluster network traffic.\n\nApplications / microservices are unaware of data plane.\n\n## Context\n\nTraditional modern architecture involves having different components exposing reusable APIs, addressing different channels (mobile, single page application, traditional server pages or B2B apps), consuming APIs (mobile APIs, back end for front end, shared common apis like authentication, authorization,...) and backend services addressing reusable business services:\n\n![](./images/current-arch-compo.png)\n\nAPI management can be added via API gateway. This is a distributed application with cross related communication channels, where any changes to the service interface characteristics impact any of the components.\n\nMoving to microservices architecture style adds more communication challenges and devops complexity but provides a lot of business values such as:\n* rapid deployment of new business capabilities, co-evolving in parallel of other services.\n* focusing on business domain with clear ownership of the business function and feature roadmap\n* better operation procedure, automated, and with easy rollout and continuous delivery.\n* A/B testing to assess how new feature deployed improve business operations\n* Improve resiliency by deploying on multi language cluster\n\nAs an example we can use the [following predictive maintenance asset solution](https://github.com/ibm-cloud-architecture/refarch-asset-analytics) with the following capabilities to support:\n\n* user authentication\n* user management: add / delete new user\n* user self registration, reset password\n* user permission control\n* user profile\n* asset management\n* risk assessment service\n\nEach could be grouped by business domain like the user management, asset management, and application access control. So domain separation can be a good microservice boundary. But if the number of user reach millions then we may need to optimize the runtime processing of reading user credential, and scale the service differently, leading to a service map like the diagram below, where runtime and management are separated services.\n\n![](./images/ms-view.png)  \n\nAll of these still does not address the fact that data are distributed and even more with microservices owning their data persistence. As developers and architects we still have to address the following data integrity problems:\n\n* two phases commit\n* compensating operation\n* eventual data consistency: some microservice updating data may share those updates with other microservices.\n* Data aggregation: adding new views on data, owned by a microservice, to support new aggregates. Examples are preparing data view for machine learning modeling, analytics, or business intelligence...\n\nFrom the previous microservice allocation we can see the needs to propagate data update between services. Adding or unsubscribing a user involves updating the asset the user own and the authentication runtime service:\n\n![](./images/data-consistency.png)  \n\nAdding a new application changes the authorization runtime service.\n\nWe are now looking at the following questions:\n\n- how does webapp access APIs for their main service, of back end for front end service.\n- how does deployed microservice access other service: discover and access?\n- How data consistency can be ensured?\n- is there a simpler way to manage cross microservice dependency?\n\nThe answers depend on the existing infrastructure and environment, and deployment needs.\n\n## Service routing\n\nWe have to dissociate intra-cluster communication versus inter clusters or cluster to external services. Without getting into too much detail of IP routing within Kubernetes some important elements of the cluster are important to remember:\n* microservices are packaged as docker container and expose port. When deployed they run in a pod within a node (physical or virtual machine)\n* containers can talk to other containers only if they are on the same machine, or when they have exposed port.\n* Kubernetes is configured with a large flat subnet (e.g. 172.30.0.0/16) which is used for internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node.\nWhen a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node.\n\n![](./images/kube-pod-network.png)   \n\n* Kube-proxy intercepts and controls where to forward the traffic, either to another worker node running your destination pod, or outside of the cluster\n* Kube proxy watches the API Server on the Master Node for the addition and removal of Services endpoints. It configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints.\n* Worker nodes have internal DNS service and load balancer\n\nWithin Kubernetes, Ingress is a service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps. You use ingress when you need to support HTTP, HTTPS, TLS, load balancing, expose app outside of the cluster, and custom routing rules...\n\nOne ingress resource is required by namespace. So if microservices are in the same namespace you can define a domain name for those services (e.g. assetmanagement.greencompute.ibmcase.com) and defined path for each service:\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: assetmanagement\nspec:\n  rules:\n    - host: assetmanagement.greencompute.ibmcase.com\n      http:\n        paths:\n          - path: /assetconsumer\n            backend:\n              serviceName: asset-consumer-svc\n              servicePort: 8080\n          - path: /assetdashboard\n            backend:\n              serviceName: asset-dashboard-bff-svc\n              servicePort: 8080\n          - path: /assetmgrms\n            backend:\n              serviceName: asset-mgr-ms-svc\n              servicePort: 8080\n```\nThe backend for front end component, the asset manager microservice and the asset consumer components are exposed in the same domain.\nThe `serviceName` matches the service exposed for each components.\nThe following diagram presents how an external application accesses deployed microservice within Kubernetes pod.\n\nThe following diagram shows how Ingress directs communication from the internet to a deployed microservice:\n\n![](./images/ingress-routing.png)\n\n1. A user sends a request to your app by accessing your app's URL. Using DNS name abstracts the application from the underlying infrastructure. Inter clusters microservice to microservice should use the same approach\n1. A DNS system service resolves the hostname in the URL to the portable public IP address of the load balancer\n1. Based on the resolved IP address, the client sends the request to the load balancer service that exposes the Application Load Balancer (ALB)\n1. The ALB checks if a routing rule for the app path in the cluster exists. If a matching rule is found, the request is forwarded according to the rules that you defined in the Ingress resource to the pod where the app is deployed. If multiple app instances are deployed in the cluster, the ALB load balances the requests between the app pods. To also load balance incoming HTTPS connections, you can configure the ALB to you can use your own TLS certificate to decrypt the network traffic.\n1. Microservice to microservice can use this DNS name to communicate between service.\n\nUsing Ingress, the global load balancer can support parallel, cross region, clusters.\n\n## Service exposition\n\nThere is an architecture style focusing on APIs which proposes to have different SLA and semantic for external, internet facing API versus internal back end APIs only exposed within intranet. [This article](../hybrid-ref-arch.md)  presents using different API gateways to support this architecture.\n\nBackend data services are not exposed directly to internet. API Gateway provides a secure end point for external web app to access those business functions.\n\nSo the decisions on how to expose service are linked to:\n\n* do you need to do API management\n* do you need to secure APIs\n* do you need to expose to internet\n* do you need to support other protocol then HTTP\n* do you need to have multiple instance of the application\n\nWhen deploying a microservice to Kubernetes it is recommended to use Ingress rule as presented above.. The following yaml file exposes the BFF service using ClusterIP:\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: asset-consumer-svc\n  labels:\n    chart: asset-consumer\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: asset-consumer-svc\n  selector:\n    app: asset-consumer\n```\n\n## Service discovery\n\nWhen deploying on Kubernetes cluster, microservices use the DNS lookup to discover deployed microservice.\n\n\n## ISTIO\n\n[ISTIO](https://istio.io/) provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, traffic flow management, monitoring, etc...\nBy deploying a special sidecar proxy (called Envoy) throughout your environment, all network communication between microservices is intercepted and controlled by ISTIO control panel.\n\nThe control plane manages the overall network infrastructure and enforces the policy and traffic rules.\n\n### More reading\n\n[Istio and Kubernetes Workshop](https://github.com/IBM/istio101/tree/master/workshop)\n[Advanced traffic management with ISTIO](https://developer.ibm.com/code/patterns/manage-microservices-traffic-using-istio/)\n[Istio workshop for IBM Cloud Container service](https://github.com/szihai/istio-workshop)\n[ Our Istio FAQ](../istio/readme#faq)\n\n## Asynchronous loosely coupled solution using events\n\nIf we change of paradigm and use a messaging approach or better an event approach of the data update requirements, we will implement a loosly coupled solution with a pub/sub communication protocol. We need to think about the activities that apply within each service and how they can be of interest to other components. Internal microservice tasks are becoming facts about something happened and those facts may be published as events for others to consume. The first level of refactoring may become:  \n\n![](./images/pub-events.png)\n\nAn event is a fact that happens in the past and carry all the data needed, and it becomes a source of record. It becoming consistent if it is played in a messaging backbone via topics.\n\n![](./images/message-topics.png)  \n\nBut the persistence of data can be externalized to consumer and then simplify the architecture:\n\n![](./images/message-topics2.png)\n\n\nThen we can use the history of the persisted events to add features not address before, and outside of the direct scope of a microservice. For example to compute the number of users added last month, just a query on the users topic will get the answer: no or very limited coding needed.\n\nWe recommend to go deeper in event driven architecture [with this site](https://ibm-cloud-architecture.github.io/refarch-eda/)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/concepts/service-mesh/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}