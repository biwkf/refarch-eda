{"componentChunkName":"component---src-pages-concepts-fit-to-purpose-index-mdx","path":"/concepts/fit-to-purpose/","result":{"pageContext":{"frontmatter":{"title":"Fit for purpose","description":"Different fit for purpose evaluation and criteria for technologies involved in EDA"},"relativePagePath":"/concepts/fit-to-purpose/index.mdx","titleType":"append","MdxNode":{"id":"ee5c26f8-0bdb-5150-9d97-f586a8f28220","children":[],"parent":"1adc8d9a-90fd-5f54-a216-c3238bade35e","internal":{"content":"---\ntitle: Fit for purpose\ndescription: Different fit for purpose evaluation and criteria for technologies involved in EDA\n---\n\nIn this note we want to list some of the main criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study.\nFit for purpose practices should be done under a bigger program about application development governance and data governance.\nWe can look at least to the following major subjects:\n\n<AnchorLinks>\n  <AnchorLink>Cloud native applications</AnchorLink>\n  <AnchorLink>Modern data pipeline</AnchorLink>\n  <AnchorLink>MQ Versus Kafka</AnchorLink>\n  <AnchorLink>Kafka Streams vs Apache Flink</AnchorLink>\n</AnchorLinks>\n\n## Cloud native applications\n\nWith the adoption of cloud native and microservice applications (the [12 factors](https://12factor.net/) app), the following needs to be addressed:\n\n* Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the '[reactive](/advantages/reactive/) manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi cloud deployment practice.\n* Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST end points to pull the data from other services, each service pushes the change to their main business entity to a event backbone. Each future service which needs those data, pulls from the messaging system.\n* Adopting common patterns like [command query responsibility seggregation](/patterns/cqrs/) to help implementing complex queries, joining different business entities owned by different microservices, [event sourcing](/patterns/event-sourcing/), [transactional outbox](/patterns/intro/#transactional-outbox) and [SAGA](/patterns/saga/) for long running transaction.\n* Addressing data eventual consistency to propagate change to other components versus ACID transaction.\n* Support \"always-on\" approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers.\n\nSupporting all or part of those requirements will lead to the adoption of event-driven microservices and architecture.\n\n## Motivation for data streaming\n\nThe central value propositions of data stream are to: \n\n* lower the cost of integrating via event streams, \n* use event streams to signal state changes in near-real time\n* replay the past to build data projection\n\nApplying the concept of data loose value over time, it is important to act on data as early\nas possible, close to creation time. After a period of time data becomes less valuable.\n\nTwo time factors are important in this data processing: **latency** (time to deliver data to consumers)\nand **retention** (time to keep data). For latency try to reduce the number of network segment between\nproducer and consumers. Considering edge computing as a way to bring event processing close to the source.\nThe event processing add time to the end to end latency. Considering constraining the processing time frame.\n\n*Retention* is a problem linked to the business requirements, and we need to assess for each topic how long\nan event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, \nprojection views... keeping for too long, increase the cost of storage, but also the time to rebuild data \nprojection. \n\n## Modern data pipeline\n\nAs new business applications need to react to events in real time, the adoption of [event backbone](/concepts/terms-and-definitions/#event-backbone) is really part of the IT toolbox. \nModern IT architecture encompasses the adoption of new data hub, where all the data about a 'customer', for example, is accessible in one event backbone. \nTherefore, it is natural to assess the data movement strategy and assess how to offload some of those ETL jobs running at night, \nby adopting real time data ingestion. \n\nWe detailed the new architecture in [this modern data lake](introduction/reference-architecture/#modern-data-lake) article, so from a *fit for purpose* point of view, \nwe need to assess the scope of existing ETL jobs, and refector to streaming logic that can be incorporated into different logs/ topics.\nWith Event Backbone like Kafka, any consumer can join the data log consumption at any point of time, within the retention period. \nBy moving the ETL logic to a streaming application, we do not need to wait for the next morning to get important metrics.\n\n## MQ Versus Kafka\n\nWe already addressed the differences between Queueing and Streaming in [this chapter](/concepts/events-versus-messages/).\n\nNow in term of technologies we can quickly highlight the followings:\n\n### Kafka characteristics\n\n* Keep message for long period of time, messages are not deleted once consumed\n* Suited for high volume and low latency processing\n* Support pub/sub model only\n* Messages are ordered in a topic/partition but not cross partitions\n* Stores and replicates events published to a topic, remove on expired period or on disk space constraint\n* Messages are removed from file system independent of applications \n* Topic can have multiple partitions to make consumer processing parallel.\n* Not supporting two phase commits / XA transaction, but message can be produced with local transaction\n* Multi-region architecture requires data replication across regions with Mirror Maker 2\n* Applications (producers, consumers, or streaming) are going to a central cluster.\n* Cluster can support multi availability zones\n* But also support extended cluster to go over different regions if those regions have low latency network\n* Scales horizontally, by adding more nodes\n* non-standard API but rich library to support the main programming language.\n* But support also HTTP bridge or proxy to get message sent via HTTP\n* when there is a problem on the broker it takes a lot of time to recover and it impacts all consumers\n* Kafka is easy to setup with kubernetes deployment with real operator, is more difficult to manage with bare metal deployment.\n* Cluster and topics definition can be managed with Gitops and automatically instantiated in new k8s cluster\n* Consumer and producers are build and deployed with simple CI/CD pipeline\n\n### MQ characteristics\n\n* Best suited for point-to-point communication \n* Supports horizontal scaling and high volume processing\n* Supports **event mesh**, where Brokers can be deployed in different environments and serve the async applications locally, \nimproving communication, performance, placement and scalability.\n* The local transaction supports to write or read a message from a queue is a strength for scalability as if a consumer is not able to\nprocess the messagem another one will do. In Kafka, one  partition is assigned to a consumer, and may be reassigned to \na new consumer after failure, which leads to a very costly work of partition rebalancing.\n* Participates to two-phase commit transaction\n* Exactly one delivery with strong consistency\n* Integrate with Mainframes: transactional applications on mainframe \n* Support JMS for JEE applications\n* Support AMQP for lighter protocol\n* Messages are removed after consumption, they stayed persisted until consumed by all subscribers\n* Strong coupling with subscribers, producer knows its consumers\n* Supports MQ brokers in cluster with leader and followers.\n* Support replication between brokers\n* Support message priority\n* Support dynamic queue creation. Typical case it replay to queue.\n* Support much more queue per broker so it is easier to scale.\n* Easily containerized and managed with Kubernetes operators.\n\n### Direct product feature comparison\n\n| Kafka | IBM MQ | \n| --- | --- |\n| Kafka is a pub/sub engine with streams and connectors | MQ is a queue,or pub/sub engine |\n| All topics are persistent \u000b | Queues and topics can be persistent or non persistent |\n| All subscribers are durable | Subscribers can be durable or non durable |\n| Adding brokers to requires little work (changing a configuration file)\u000b\u000b| Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels.  Queues and Topics need to be added to the cluster.) |\n| Topics can be spread across brokers (partitions) with a command\u000b | Queues and topics can be spread across a cluster by adding them to clustered QMGRs |\n| Producers and Consumers are aware of changes made to the cluster | All MQ clients require a CCDT file to know of changes if not using a gateway QMGR |\n| Can have n number of replication partitions | Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs |\n| Simple load balancing\u000b | Load balancing can be simple or more complex using weights and affinity |\n| Can reread messages\u000b | Cannot reread messages that have been already processed | \n| All clients connect using a single connection method | MQ has Channels which allow different clients to connect, each having the ability to have different security requirements | \n| Data Streams processing built in, using Kafka topic for efficiency| Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. |\n| Has connection security, authentication security, and ACLs (read/write to Topic) | Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) |\n| Built on Java, so can run on any platform that support Java 8+ | Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container | \n| Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus | Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc) |\n\n### Migrating from MQ to Kafka\n\nWhen the real conditions as listed above are met, architects may assess if it makes sense to migrate MQ application to Kafka. \nMost of the time the investment is not justified. Modern MQ supports the same DevOps and deployment pattern as other cloud native applications.\n\nJEE or mainframe applications use MQ in transaction to avoid duplicate messages or loss of messages. Supporting exactly once delivery in Kafka\nneeds some configuration and participation of producer and consumers: far more complex to implement.\n\nWe recommend adopting the two messaging capabilities for any business applications.\n\n## Kafka Streams vs Apache Flink\n\nOnce we have setup data streams, we need technology to support near real-time analytics and complex event processing. Historically, analytics performed on static data was done using batch reporting techniques. However, if\ninsights have to be derived in near real-time, event-driven architectures help to analyse and look for patterns within events.\n\n[Apache Flink](https://flink.apache.org) (2016) is a framework and **distributed processing** engine for stateful computations over unbounded and bounded data streams. It is considered to be superior to Apache Spark and Hadoop. It supports batch and graph processing and complex event processing. \nThe major stream processing features offered by Flink are:\n\n* Support for event time and out of order streams: use event time for consistent results\n* Consistency, fault tolerance, and high availability: guarantees consistent state updates in the presence of failures and consistent data movement between selected sources and sinks\n* Low latency and high throughput: tune the latency-throughput trade off, making the system suitable for both high-throughput data ingestion and transformations, as well as ultra low latency (millisecond range) applications.\n* Expressive and easy-to-use APIs in Scala and Java: map, reduce, join, with window, split,... Easy to implement the business logic using Function.\n* Support for sessions and unaligned windows: Flink completely decouples windowing from fault tolerance, allowing for richer forms of windows, such as sessions.\n* Connectors and integration points: Kafka, Kinesis, Queue, Database, Devices...\n* Developer productivity and operational simplicity: Start in IDE to develop and deploy and deploy to Kubernetes, [Yarn](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html), [Mesos](http://mesos.apache.org/) or containerized\n* Support batch processing\n* Includes Complex Event Processing capabilities\n\nHere is simple diagram of Flink architecture from the Flink web site:\n\n ![Flink components](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/distributed-runtime.svg)\n\n\nSee this [technology summary](/technology/flink/).\n\nSee also [this article from Confluent](https://www.confluent.io/blog/apache-flink-apache-kafka-streams-comparison-guideline-users) about comparing with Kafka Streams.","type":"Mdx","contentDigest":"1dd7f17bd475bbcb1d4e4e2eec9faa1e","owner":"gatsby-plugin-mdx","counter":841},"frontmatter":{"title":"Fit for purpose","description":"Different fit for purpose evaluation and criteria for technologies involved in EDA"},"exports":{},"rawBody":"---\ntitle: Fit for purpose\ndescription: Different fit for purpose evaluation and criteria for technologies involved in EDA\n---\n\nIn this note we want to list some of the main criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study.\nFit for purpose practices should be done under a bigger program about application development governance and data governance.\nWe can look at least to the following major subjects:\n\n<AnchorLinks>\n  <AnchorLink>Cloud native applications</AnchorLink>\n  <AnchorLink>Modern data pipeline</AnchorLink>\n  <AnchorLink>MQ Versus Kafka</AnchorLink>\n  <AnchorLink>Kafka Streams vs Apache Flink</AnchorLink>\n</AnchorLinks>\n\n## Cloud native applications\n\nWith the adoption of cloud native and microservice applications (the [12 factors](https://12factor.net/) app), the following needs to be addressed:\n\n* Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the '[reactive](/advantages/reactive/) manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi cloud deployment practice.\n* Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST end points to pull the data from other services, each service pushes the change to their main business entity to a event backbone. Each future service which needs those data, pulls from the messaging system.\n* Adopting common patterns like [command query responsibility seggregation](/patterns/cqrs/) to help implementing complex queries, joining different business entities owned by different microservices, [event sourcing](/patterns/event-sourcing/), [transactional outbox](/patterns/intro/#transactional-outbox) and [SAGA](/patterns/saga/) for long running transaction.\n* Addressing data eventual consistency to propagate change to other components versus ACID transaction.\n* Support \"always-on\" approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers.\n\nSupporting all or part of those requirements will lead to the adoption of event-driven microservices and architecture.\n\n## Motivation for data streaming\n\nThe central value propositions of data stream are to: \n\n* lower the cost of integrating via event streams, \n* use event streams to signal state changes in near-real time\n* replay the past to build data projection\n\nApplying the concept of data loose value over time, it is important to act on data as early\nas possible, close to creation time. After a period of time data becomes less valuable.\n\nTwo time factors are important in this data processing: **latency** (time to deliver data to consumers)\nand **retention** (time to keep data). For latency try to reduce the number of network segment between\nproducer and consumers. Considering edge computing as a way to bring event processing close to the source.\nThe event processing add time to the end to end latency. Considering constraining the processing time frame.\n\n*Retention* is a problem linked to the business requirements, and we need to assess for each topic how long\nan event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, \nprojection views... keeping for too long, increase the cost of storage, but also the time to rebuild data \nprojection. \n\n## Modern data pipeline\n\nAs new business applications need to react to events in real time, the adoption of [event backbone](/concepts/terms-and-definitions/#event-backbone) is really part of the IT toolbox. \nModern IT architecture encompasses the adoption of new data hub, where all the data about a 'customer', for example, is accessible in one event backbone. \nTherefore, it is natural to assess the data movement strategy and assess how to offload some of those ETL jobs running at night, \nby adopting real time data ingestion. \n\nWe detailed the new architecture in [this modern data lake](introduction/reference-architecture/#modern-data-lake) article, so from a *fit for purpose* point of view, \nwe need to assess the scope of existing ETL jobs, and refector to streaming logic that can be incorporated into different logs/ topics.\nWith Event Backbone like Kafka, any consumer can join the data log consumption at any point of time, within the retention period. \nBy moving the ETL logic to a streaming application, we do not need to wait for the next morning to get important metrics.\n\n## MQ Versus Kafka\n\nWe already addressed the differences between Queueing and Streaming in [this chapter](/concepts/events-versus-messages/).\n\nNow in term of technologies we can quickly highlight the followings:\n\n### Kafka characteristics\n\n* Keep message for long period of time, messages are not deleted once consumed\n* Suited for high volume and low latency processing\n* Support pub/sub model only\n* Messages are ordered in a topic/partition but not cross partitions\n* Stores and replicates events published to a topic, remove on expired period or on disk space constraint\n* Messages are removed from file system independent of applications \n* Topic can have multiple partitions to make consumer processing parallel.\n* Not supporting two phase commits / XA transaction, but message can be produced with local transaction\n* Multi-region architecture requires data replication across regions with Mirror Maker 2\n* Applications (producers, consumers, or streaming) are going to a central cluster.\n* Cluster can support multi availability zones\n* But also support extended cluster to go over different regions if those regions have low latency network\n* Scales horizontally, by adding more nodes\n* non-standard API but rich library to support the main programming language.\n* But support also HTTP bridge or proxy to get message sent via HTTP\n* when there is a problem on the broker it takes a lot of time to recover and it impacts all consumers\n* Kafka is easy to setup with kubernetes deployment with real operator, is more difficult to manage with bare metal deployment.\n* Cluster and topics definition can be managed with Gitops and automatically instantiated in new k8s cluster\n* Consumer and producers are build and deployed with simple CI/CD pipeline\n\n### MQ characteristics\n\n* Best suited for point-to-point communication \n* Supports horizontal scaling and high volume processing\n* Supports **event mesh**, where Brokers can be deployed in different environments and serve the async applications locally, \nimproving communication, performance, placement and scalability.\n* The local transaction supports to write or read a message from a queue is a strength for scalability as if a consumer is not able to\nprocess the messagem another one will do. In Kafka, one  partition is assigned to a consumer, and may be reassigned to \na new consumer after failure, which leads to a very costly work of partition rebalancing.\n* Participates to two-phase commit transaction\n* Exactly one delivery with strong consistency\n* Integrate with Mainframes: transactional applications on mainframe \n* Support JMS for JEE applications\n* Support AMQP for lighter protocol\n* Messages are removed after consumption, they stayed persisted until consumed by all subscribers\n* Strong coupling with subscribers, producer knows its consumers\n* Supports MQ brokers in cluster with leader and followers.\n* Support replication between brokers\n* Support message priority\n* Support dynamic queue creation. Typical case it replay to queue.\n* Support much more queue per broker so it is easier to scale.\n* Easily containerized and managed with Kubernetes operators.\n\n### Direct product feature comparison\n\n| Kafka | IBM MQ | \n| --- | --- |\n| Kafka is a pub/sub engine with streams and connectors | MQ is a queue,or pub/sub engine |\n| All topics are persistent \u000b | Queues and topics can be persistent or non persistent |\n| All subscribers are durable | Subscribers can be durable or non durable |\n| Adding brokers to requires little work (changing a configuration file)\u000b\u000b| Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels.  Queues and Topics need to be added to the cluster.) |\n| Topics can be spread across brokers (partitions) with a command\u000b | Queues and topics can be spread across a cluster by adding them to clustered QMGRs |\n| Producers and Consumers are aware of changes made to the cluster | All MQ clients require a CCDT file to know of changes if not using a gateway QMGR |\n| Can have n number of replication partitions | Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs |\n| Simple load balancing\u000b | Load balancing can be simple or more complex using weights and affinity |\n| Can reread messages\u000b | Cannot reread messages that have been already processed | \n| All clients connect using a single connection method | MQ has Channels which allow different clients to connect, each having the ability to have different security requirements | \n| Data Streams processing built in, using Kafka topic for efficiency| Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. |\n| Has connection security, authentication security, and ACLs (read/write to Topic) | Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) |\n| Built on Java, so can run on any platform that support Java 8+ | Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container | \n| Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus | Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc) |\n\n### Migrating from MQ to Kafka\n\nWhen the real conditions as listed above are met, architects may assess if it makes sense to migrate MQ application to Kafka. \nMost of the time the investment is not justified. Modern MQ supports the same DevOps and deployment pattern as other cloud native applications.\n\nJEE or mainframe applications use MQ in transaction to avoid duplicate messages or loss of messages. Supporting exactly once delivery in Kafka\nneeds some configuration and participation of producer and consumers: far more complex to implement.\n\nWe recommend adopting the two messaging capabilities for any business applications.\n\n## Kafka Streams vs Apache Flink\n\nOnce we have setup data streams, we need technology to support near real-time analytics and complex event processing. Historically, analytics performed on static data was done using batch reporting techniques. However, if\ninsights have to be derived in near real-time, event-driven architectures help to analyse and look for patterns within events.\n\n[Apache Flink](https://flink.apache.org) (2016) is a framework and **distributed processing** engine for stateful computations over unbounded and bounded data streams. It is considered to be superior to Apache Spark and Hadoop. It supports batch and graph processing and complex event processing. \nThe major stream processing features offered by Flink are:\n\n* Support for event time and out of order streams: use event time for consistent results\n* Consistency, fault tolerance, and high availability: guarantees consistent state updates in the presence of failures and consistent data movement between selected sources and sinks\n* Low latency and high throughput: tune the latency-throughput trade off, making the system suitable for both high-throughput data ingestion and transformations, as well as ultra low latency (millisecond range) applications.\n* Expressive and easy-to-use APIs in Scala and Java: map, reduce, join, with window, split,... Easy to implement the business logic using Function.\n* Support for sessions and unaligned windows: Flink completely decouples windowing from fault tolerance, allowing for richer forms of windows, such as sessions.\n* Connectors and integration points: Kafka, Kinesis, Queue, Database, Devices...\n* Developer productivity and operational simplicity: Start in IDE to develop and deploy and deploy to Kubernetes, [Yarn](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html), [Mesos](http://mesos.apache.org/) or containerized\n* Support batch processing\n* Includes Complex Event Processing capabilities\n\nHere is simple diagram of Flink architecture from the Flink web site:\n\n ![Flink components](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/distributed-runtime.svg)\n\n\nSee this [technology summary](/technology/flink/).\n\nSee also [this article from Confluent](https://www.confluent.io/blog/apache-flink-apache-kafka-streams-comparison-guideline-users) about comparing with Kafka Streams.","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/concepts/fit-to-purpose/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}